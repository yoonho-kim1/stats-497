{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18afad0",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d26659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tsmoothie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ab482",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n",
    "from bioinfokit.analys import stat\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00461d71",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../output/train_df.pkl')\n",
    "test_df = pd.read_pickle('../output/test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f494d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b186e8",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_weekend(day):\n",
    "    \"\"\"This function will group days into weekends. Weekends are defined as Friday, Saturday, and Sunday. \n",
    "    Otherwise, it will be grouped as weekdays.\n",
    "    Input: \n",
    "    day -> string (names of days)\n",
    "    \n",
    "    Output: \n",
    "    new_days -> int (boolean values for weekend or not)\"\"\"\n",
    "    \n",
    "    # clean day names\n",
    "    lower_day = str(day).lower().strip()\n",
    "    # define weekend\n",
    "    weekends = ['friday', 'saturday', 'sunday']\n",
    "    if lower_day in weekends:\n",
    "        new_days = 1\n",
    "    else:\n",
    "        new_days = 0\n",
    "    \n",
    "    return new_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_last_day(data):\n",
    "    \"\"\"This function will help us identify the last of day each month. \n",
    "    This is needed because Feburary can have different length in leap years.\n",
    "    We need to identify both 15th and the last day of each month because\n",
    "    that's when employees get paid in Ecuador.\n",
    "    Input: \n",
    "    date -> int (individual dates)\n",
    "    \n",
    "    Output: \n",
    "    res -> int (last day of each month)\"\"\"\n",
    "    \n",
    "    n = len(data)\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        last_day = calendar.monthrange(data['date_year'][i], data['date_month'][i])[1]\n",
    "        res.append(last_day)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50bb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_payday(data):\n",
    "    \"\"\"This function will help us identify 15th and the last day of each month.\n",
    "    Input: \n",
    "    data -> dataframe\n",
    "    \n",
    "    Output: \n",
    "    data -> dataframe (after we get boolean values to indicate if paydays or not)\"\"\"\n",
    "    \n",
    "    cond = (data['date_day'] == 15) | (data['date_day'] == data['last_day'])\n",
    "    data.loc[:, ''] = np.where(cond, 1, 0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_range(data, num):\n",
    "    \"\"\"This function will capture time frames for given dates.\n",
    "    Input: \n",
    "    data -> dataframe (original dataframe)\n",
    "    num -> int (specify the time frames for the target dates)\n",
    "    \n",
    "    Output: \n",
    "    new_date -> datetime (a range of dates from the given dates)\"\"\"\n",
    "    \n",
    "    target_range = timedelta(days = num)\n",
    "    \n",
    "    date['date'] = pd.to_datetime(data['date'])\n",
    "    data.loc[:, 'added_dates'] = data['date'] + target_range\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_time_range(data, num_days):\n",
    "    \"\"\"This function will help identify the difference in terms of sales in 'Transferred' holidays.\n",
    "    Input:\n",
    "    data -> dataframe\n",
    "    num_days -> time windows that we want to focus in (int)\n",
    "    \n",
    "    Output:\n",
    "    new_data -> dataframe (this dataframe will contain information \n",
    "    regarding different dates and their average sales for transferred holidays)\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting transferred holidays and sales only\n",
    "    new_data = data[['date', 'transferred', 'sales']]\n",
    "    \n",
    "    # reset index so that we can loop through it\n",
    "    new_data.reset_index(inplace = True)\n",
    "    new_data.drop(columns = {'index'}, inplace = True)\n",
    "    \n",
    "    # create a time frame we want to look into\n",
    "    time_range = pd.timedelta_range('1 day', periods = num_days)\n",
    "    \n",
    "    # create an aggregated values for different dates\n",
    "    sales_summary = new_data.groupby(['date']).mean()[['sales']].reset_index()\n",
    "    \n",
    "    # change the summary dataframe as dictionary so that we can use mapper\n",
    "    \n",
    "    date_mapper = {}\n",
    "    unique_dates = list(sales_summary.date.unique())\n",
    "    \n",
    "    for date in unique_dates:\n",
    "        date_mapper[date] = sales_summary[sales_summary.date == date]['sales'].values[0]\n",
    "        \n",
    "    for i in range(len(time_range)):\n",
    "        \n",
    "        new_data.loc[:, f'-{i+1}_delta'] = new_data['date'] - time_range[i]\n",
    "        new_data.loc[:, f'-{i+1}_delta_sales'] = new_data.loc[:, f'-{i+1}_delta'].map(date_mapper)\n",
    "        new_data.loc[:, f'+{i+1}_delta'] = new_data['date'] + time_range[i]\n",
    "        new_data.loc[:, f'+{i+1}_delta_sales'] = new_data.loc[:, f'+{i+1}_delta'].map(date_mapper)\n",
    "        \n",
    "    new_data = new_data[new_data['transferred'] == True]\n",
    "        \n",
    "    return new_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e73736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_the_means(data):\n",
    "    \"\"\"This function will find the difference between transferred holidays and their mean sales.\n",
    "    Input: \n",
    "    data -> datafrmae (the dataframe we created for delta dates)\n",
    "    \n",
    "    Output:\n",
    "    sales_compare -> dictionary \n",
    "    (the dictionary will contain information regarding\n",
    "    the original dates and delta dates in terms of average sales)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create a dictionary that contains information of different dates sales\n",
    "    sales_compare = {}\n",
    "    \n",
    "    # get unique transferred holidays from the dataframe\n",
    "    unique_holiday  = list(data['date'].unique())\n",
    "    \n",
    "    # select features that have information regarding the average sales.\n",
    "    sales_features = [col for col in data.columns if 'sales' in col]\n",
    "    \n",
    "    for date in unique_holiday:\n",
    "        # filter out a specific transferred holidays\n",
    "        filtered_data = data[data['date'] == date]\n",
    "        \n",
    "        feature_dict = {}\n",
    "        \n",
    "        for feature in sales_features:\n",
    "            if feature != 'sales':\n",
    "                # compare the results between original dates and delta dates\n",
    "                results = ttest_ind(filtered_data['sales'],\n",
    "                                    filtered_data[feature].dropna())\n",
    "                p_value = results[-1]\n",
    "                \n",
    "                # only save the results when the p-value is less than 0.05\n",
    "                if p_value < 0.05:\n",
    "                    feature_dict[feature] = results\n",
    "                sales_compare[date] = feature_dict\n",
    "    return sales_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c97702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_delta_date(data, days):\n",
    "    \n",
    "    \"\"\"This function will find time delta for specific date.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    days -> list (list of days that we want to find on a given date)\n",
    "    \n",
    "    Output:\n",
    "    data -> dataframe (dataframe that contains timedelta information using days)\"\"\"\n",
    "    \n",
    "    for day in days:\n",
    "        data.loc[:, f\"delta_{day}\"] = data.apply(lambda row: row['date'] + timedelta(days = day) if row['transferred'] == True else row['date'], axis = 1)\n",
    "     \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c144f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dates_for_delta(data):\n",
    "    \n",
    "    \"\"\"This function will compare the actual date and delta dates for those transfer holidays.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    \n",
    "    Output:\n",
    "    data -> dataframe (after identfying deltas)\"\"\"\n",
    "    \n",
    "    delta_features = [col for col in data.columns if 'delta' in col]\n",
    "    \n",
    "    for col in delta_features:\n",
    "        if col == 'payday_delta':\n",
    "            pass\n",
    "        else:\n",
    "            delta_list = list(data[data.date != data[col]][col].unique())\n",
    "            data.loc[:, f\"is_{col}\"] = data.date.apply(lambda x: 1 if x in delta_list else 0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28819fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_christmas_sales(christmas_list):\n",
    "    \n",
    "    \"\"\"This function will find Christmas within the dataframe.\n",
    "    \n",
    "    Input:\n",
    "    christmas_list -> list (a list of Christmas in different years)\n",
    "    \n",
    "    Output:\n",
    "    date_list -> list (a range of Christmas season based on the input list)\"\"\"\n",
    "    \n",
    "    date_list = []\n",
    "    # change string to datetime object\n",
    "    christmas_list = [pd.to_datetime(d) for d in christmas_list]\n",
    "    datedelta_list = [-5, -4, -3, -2, -1]\n",
    "    \n",
    "    \n",
    "    for christmas in christmas_list:\n",
    "        for delta in datedelta_list:\n",
    "            target_range = timedelta(days = delta)\n",
    "            christmas_delta = christmas + target_range\n",
    "            date_list.append(christmas_delta)\n",
    "            \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42d6e9",
   "metadata": {},
   "source": [
    "### Check dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25899163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.isnull().sum()[train_df.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e3e35",
   "metadata": {},
   "source": [
    "There are some columns that have missing values. This might happen due to the join issue since we used left join. Let's take a look at the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d11782",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02227eda",
   "metadata": {},
   "source": [
    "We will create more columns after finishing our EDA, but for now, we can create few columns from the original columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db257d",
   "metadata": {},
   "source": [
    "We can change the columns to more proper data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e59d94",
   "metadata": {},
   "source": [
    "###### date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3547a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'date'] = pd.DatetimeIndex(train_df.date)\n",
    "test_df.loc[:, 'date'] = pd.DatetimeIndex(test_df.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419ea6e",
   "metadata": {},
   "source": [
    "Change the data type for date from object to date. This will allow us to manipulate data more easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec8c21",
   "metadata": {},
   "source": [
    "getting year, quarter, month, and days from the date column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55debe9a",
   "metadata": {},
   "source": [
    "- date manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae99e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'date_year'] = train_df.date.dt.year\n",
    "train_df.loc[:, 'date_quarter'] = train_df.date.dt.quarter\n",
    "train_df.loc[:, 'date_month'] = train_df.date.dt.month\n",
    "train_df.loc[:, 'date_day'] = train_df.date.dt.day\n",
    "train_df.loc[:, 'date_week'] = train_df.date.dt.week\n",
    "train_df.loc[:, 'date_day_name'] = train_df.date.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'date_year'] = test_df.date.dt.year\n",
    "test_df.loc[:, 'date_quarter'] = test_df.date.dt.quarter\n",
    "test_df.loc[:, 'date_month'] = test_df.date.dt.month\n",
    "test_df.loc[:, 'date_day'] = test_df.date.dt.day\n",
    "test_df.loc[:, 'date_week'] = test_df.date.dt.week\n",
    "test_df.loc[:, 'date_day_name'] = test_df.date.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'year_month'] = train_df.date.apply(lambda x: str(x)[:7])\n",
    "test_df.loc[:, 'year_month'] = test_df.date.apply(lambda x: str(x)[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb45e5c",
   "metadata": {},
   "source": [
    "Getting information from those individual dates including year, quarter, month, day and name of the days. Keep in mind that in the data description section, people get paid on the 15th and the last day of the month. Maybe we can check those days or the day after to see if there is a seasonality. We can also group Friday, Saturday, and Sunday as Weekend, and put others as Weekday. Let's create two columns from the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479dd190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 6))\n",
    "plt.title('Historical Sales Data', fontsize = 18)\n",
    "sns.lineplot(x = 'date',\n",
    "             y = 'sales',\n",
    "             data = train_df)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.axhline(np.mean(train_df['sales']), color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9856d5",
   "metadata": {},
   "source": [
    "The red line shows the average sales throughout the years in the dataset. There seems to be a trend in the dataset. In the year of 2013, the company did not seem to perform that well, but the company performs better starting from the year of 2015. Looks like there is an increase at the end of years and a decrease in the beginning of years. Therefore, let's take a closer look in terms of months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "plt.title(\"Sales by Quarter\", fontsize = 18)\n",
    "sns.barplot(x = 'date_quarter',\n",
    "            hue = 'date_year',\n",
    "            color = 'blue',\n",
    "            alpha = 0.7,\n",
    "            y = 'sales',\n",
    "            data = train_df)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.axhline(np.mean(train_df['sales']), color = 'red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 10))\n",
    "plt.title('Historical Sales Data', fontsize = 18)\n",
    "sns.lineplot(x = 'year_month',\n",
    "             y = 'sales',\n",
    "             data = train_df)\n",
    "plt.xticks(fontsize = 12, rotation = 270)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.axhline(np.mean(train_df['sales']), color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbc39a",
   "metadata": {},
   "source": [
    "oil price and sales usign scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb892c3",
   "metadata": {},
   "source": [
    "is correlation good measure of data? data needs to be linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580c9a7",
   "metadata": {},
   "source": [
    "first thing is to plot the data before looking into correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a24232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.groupby('date_month').mean()[['sales']].sort_values(by = 'sales', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61499956",
   "metadata": {},
   "source": [
    "- weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8248211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'is_weekend'] = train_df.date_day_name.apply(group_weekend)\n",
    "test_df.loc[:, 'is_weekend'] = test_df.date_day_name.apply(group_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec56170",
   "metadata": {},
   "source": [
    "Find weekend days. If weekend, then 1 else 0. Please refer to the function in the helper section for detailed description. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdc976",
   "metadata": {},
   "source": [
    "check if there is a difference between weekdays and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa59d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('is_weekend').mean()[['sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(train_df[train_df.is_weekend == 1]['sales'],\n",
    "          train_df[train_df.is_weekend == 0]['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1b824",
   "metadata": {},
   "source": [
    "Based on the test, there is a difference between the weekdays and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot.loc[:, 'to_actual_weekends'] = train_plot.is_weekend.apply(lambda x: 'weekend' if x == 1 else 'weekdays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b902ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "plt.title(\"Sales: Weekdays vs Weekend\", fontsize = 18)\n",
    "sns.barplot(x = 'to_actual_weekends',\n",
    "            color = 'blue',\n",
    "            alpha = 0.7,\n",
    "            y = 'sales',\n",
    "            data = train_plot)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.axhline(np.mean(train_df['sales']), color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b99799",
   "metadata": {},
   "source": [
    "- payday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'last_day'] = find_the_last_day(train_df)\n",
    "test_df.loc[:, 'last_day'] = find_the_last_day(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c67735",
   "metadata": {},
   "source": [
    "Find the last day of each month first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'is_payday'] = np.where((train_df['date_day'] == 15) | (train_df['date_day'] == train_df['last_day']), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d72d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[:, 'is_payday'] = np.where((test_df['date_day'] == 15) | (test_df['date_day'] == test_df['last_day']), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f189b7c",
   "metadata": {},
   "source": [
    "Getting the payday using the last day or the 15th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e594311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('is_payday').mean()[['sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756dfec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(train_df[train_df.is_payday == 1]['sales'],\n",
    "          train_df[train_df.is_payday == 0]['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86937a5",
   "metadata": {},
   "source": [
    "looks like there is no difference between payday and others. we can take a look at the total sales and try to capture the range after the payday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c48c7",
   "metadata": {},
   "source": [
    "Let's take the payday within the partition and try to see if sales within a week timeframe would be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'actual_payday'] = train_df.apply(lambda row: row['date'] if row['is_payday'] == 1 else None, axis = 1)\n",
    "test_df.loc[:, 'actual_payday'] = test_df.apply(lambda row: row['date'] if row['is_payday'] == 1 else None, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7048e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.actual_payday.fillna(method ='ffill', inplace = True)\n",
    "test_df.actual_payday.fillna(method ='ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523ec8a",
   "metadata": {},
   "source": [
    "Even after finding the actual pay day, there are missing values. This is because the first payday partition is in Junuary 1st. We can impute those missing values as 2012-12-31. For the test dataset, we can impute 8-15-22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'actual_payday'] = train_df.actual_payday.fillna(pd.to_datetime('2012-12-31'))\n",
    "test_df.loc[:, 'actual_payday'] = test_df.actual_payday.fillna(pd.to_datetime('2017-08-15'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'payday_delta'] = train_df.date - train_df.actual_payday\n",
    "test_df.loc[:, 'payday_delta'] = test_df.date - test_df.actual_payday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7d855",
   "metadata": {},
   "source": [
    "payday delta should range from 0 day (the date when people get paid) to 15 days (to the next paycheck period since people get paid two times per month in Ecuador). The data manipulation makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a02557",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('payday_delta').mean()[['sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b1c7ea",
   "metadata": {},
   "source": [
    "Looks like different days have different sales, but how can we confirm that? Let's use Anova to see if there is any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stat()\n",
    "res.anova_stat(df = train_df, \n",
    "               res_var = 'sales', \n",
    "               anova_model = 'sales ~ C(payday_delta)')\n",
    "res.anova_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb495a",
   "metadata": {},
   "source": [
    "Looks like there might be at least one difference between these groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343e5b9",
   "metadata": {},
   "source": [
    "Using the summary statistics, let's get the average value of these dates, and divide them into 3 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('payday_delta').mean()[['sales']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404befd4",
   "metadata": {},
   "source": [
    "Cleaned our group differences and let's use the summary statistics to find out the cut off. Let's use the median (50%) as our cut off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_median = train_df.groupby('payday_delta').mean()[['sales']].describe().T['50%'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07555b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "payday_mapper = train_df.groupby('payday_delta').mean()[['sales']] > sales_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "payday_mapper.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "payday_mapper.rename(columns= {'sales':'is_above_median'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe0ac2",
   "metadata": {},
   "source": [
    "Based on the observation, looks like people normally do grocery within 7 days after they get paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f149f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_list = list(payday_mapper[payday_mapper.is_above_median == True]['payday_delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e69294",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'is_above_median'] = train_df.payday_delta.apply(lambda x: 1 if x in delta_list else 0)\n",
    "test_df.loc[:, 'is_above_median'] = test_df.payday_delta.apply(lambda x: 1 if x in delta_list else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bcf42",
   "metadata": {},
   "source": [
    "###### store_nbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_avg = train_df.groupby('store_nbr').mean()[['sales']].sort_values(by = 'sales', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_avg.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7bec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 8))\n",
    "plt.title('Sales Based on Stores', fontsize = 20)\n",
    "store_avg_bar = sns.barplot(x = 'store_nbr', \n",
    "            y = 'sales',\n",
    "            color = 'blue',\n",
    "            alpha = 0.6,\n",
    "            data = store_sales_avg)\n",
    "plt.xticks(fontsize = 20, rotation = 270)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "store_avg_bar.axhline(np.mean(store_sales_avg)['sales'], linewidth = 3, color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152eebe",
   "metadata": {},
   "source": [
    "The plot above shows the average of sales based on different stores. Using the summary statistics, let's re group them based on their sales. We can use this summary to put stores into different bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_q = store_sales_avg['sales'].describe()['25%']\n",
    "third_q = store_sales_avg['sales'].describe()['75%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_avg.loc[:, 'store_sales_bins'] = store_sales_avg.sales.apply(lambda x: 'low' if x < first_q else ('avg' if x < third_q else 'high'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6841b",
   "metadata": {},
   "source": [
    "Using this information, let's regroup stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a660ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(store_sales_avg[['store_nbr', 'store_sales_bins']],\n",
    "               left_on = 'store_nbr',\n",
    "               right_on = 'store_nbr',\n",
    "               how = 'left')\n",
    "\n",
    "test_df = test_df.merge(store_sales_avg[['store_nbr', 'store_sales_bins']],\n",
    "               left_on = 'store_nbr',\n",
    "               right_on = 'store_nbr',\n",
    "               how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945eea1",
   "metadata": {},
   "source": [
    "###### family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d562f6",
   "metadata": {},
   "source": [
    "Similarly, we can apply the same logic that we created in the store_nbr to the family column. We will use 25% and 75% percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_avg_sales = train_df.groupby('family').mean()[['sales']].sort_values(by = 'sales', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b32858",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_avg_sales.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_first_q = family_avg_sales['sales'].describe()['25%']\n",
    "family_third_q = family_avg_sales['sales'].describe()['75%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_avg_sales.loc[:, 'family_sales_bins'] = family_avg_sales.sales.apply(lambda x: 'low' if x < family_first_q else ('avg' if x < family_third_q else 'high'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e109e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(family_avg_sales[['family', 'family_sales_bins']],\n",
    "               left_on = 'family',\n",
    "               right_on = 'family',\n",
    "               how = 'left')\n",
    "\n",
    "test_df = test_df.merge(family_avg_sales[['family', 'family_sales_bins']],\n",
    "               left_on = 'family',\n",
    "               right_on = 'family',\n",
    "               how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e163bbb",
   "metadata": {},
   "source": [
    "###### onpromotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c320b85",
   "metadata": {},
   "source": [
    "This feature presents the number of total items that were on promotion on a given date. We can use state, city, family, and the store numbers to find out the summary statistics for average promoted items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaf46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onpromo_avg = train_df.groupby(['state', 'city', 'family', 'store_nbr']).mean()[['onpromotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c44661",
   "metadata": {},
   "outputs": [],
   "source": [
    "onpromo_avg.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "onpromo_avg.loc[:, 'unique_key'] = onpromo_avg.state.apply(lambda x: str(x).lower().strip()) + '-' + onpromo_avg.city.apply(lambda x: str(x).lower().strip()) + '-' + onpromo_avg.family.apply(lambda x: str(x).lower().strip()) + '-' + onpromo_avg.store_nbr.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec748d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'unique_key'] = train_df.state.apply(lambda x: str(x).lower().strip()) + '-' + train_df.city.apply(lambda x: str(x).lower().strip()) + '-' + train_df.family.apply(lambda x: str(x).lower().strip()) + '-' + train_df.store_nbr.apply(lambda x: str(x))\n",
    "test_df.loc[:, 'unique_key'] = test_df.state.apply(lambda x: str(x).lower().strip()) + '-' + test_df.city.apply(lambda x: str(x).lower().strip()) + '-' + test_df.family.apply(lambda x: str(x).lower().strip()) + '-' + test_df.store_nbr.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "onpromo_avg.rename(columns= {'onpromotion':'onpromotion_avg'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(onpromo_avg[['unique_key', 'onpromotion_avg']],\n",
    "               left_on = 'unique_key',\n",
    "               right_on = 'unique_key',\n",
    "               how = 'left')\n",
    "\n",
    "test_df = test_df.merge(onpromo_avg[['unique_key', 'onpromotion_avg']],\n",
    "               left_on = 'unique_key',\n",
    "               right_on = 'unique_key',\n",
    "               how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22328e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'onpromo_avg_bins'] = train_df.apply(lambda row: 'higher_than_avg' if row['onpromotion'] > row['onpromotion_avg'] else 'lower_than_avg', axis = 1)\n",
    "test_df.loc[:, 'onpromo_avg_bins'] = test_df.apply(lambda row: 'higher_than_avg' if row['onpromotion'] > row['onpromotion_avg'] else 'lower_than_avg', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a110e66",
   "metadata": {},
   "source": [
    "Using information above, we can recode the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002edae0",
   "metadata": {},
   "source": [
    "###### dcoilwtico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdaffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.title(\"Oil Price Change in Ecuador\", fontsize = 16)\n",
    "oil_line = sns.lineplot(x = 'date',\n",
    "             y = 'dcoilwtico',\n",
    "             color = 'blue',\n",
    "             alpha = 0.7,\n",
    "             data = train_df)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "oil_line.axhline(np.mean(train_df['dcoilwtico']), linewidth = 3, color = 'red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4645eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dcoilwtico.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aaa163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.dcoilwtico.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dcoilwtico.fillna(method = 'ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2761911",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.dcoilwtico.fillna(method = 'ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9589852",
   "metadata": {},
   "source": [
    "fill out missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price = pd.read_csv('../output/complete_oil.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price.loc[:, 'previous_price'] = oil_price.dcoilwtico.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6446a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price.loc[:, 'price_indicator'] = oil_price.apply(lambda row: 'decreased' if row['dcoilwtico'] > row['previous_price'] else 'increased_or_same', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price.loc[:, 'year'] = oil_price.date.apply(lambda x: x[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_summary_stat = oil_price.groupby('year').describe()[['dcoilwtico']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba25f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_dict = {}\n",
    "\n",
    "unique_year = list(oil_summary_stat.year.values)\n",
    "\n",
    "for year in unique_year:\n",
    "    if year not in oil_dict:\n",
    "        oil_dict[year] = oil_summary_stat[oil_summary_stat.year == year][('dcoilwtico',   'mean')].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035af3b",
   "metadata": {},
   "source": [
    "this dictionary contains years and their average oil price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea13ec6",
   "metadata": {},
   "source": [
    "1. bring the price indicator for the gas price\n",
    "2. using the average gas price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'date_join'] = train_df.date.apply(lambda x: str(x)[:10])\n",
    "test_df.loc[:, 'date_join'] = test_df.date.apply(lambda x: str(x)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0d884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.merge(oil_price[['date','price_indicator']], \n",
    "               left_on = 'date_join',\n",
    "               right_on = 'date',\n",
    "               how = 'left')\n",
    "\n",
    "\n",
    "test_df = test_df.merge(oil_price[['date','price_indicator']], \n",
    "               left_on = 'date_join',\n",
    "               right_on = 'date',\n",
    "               how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.price_indicator.fillna('increased_or_same', inplace = True)\n",
    "test_df.price_indicator.fillna('increased_or_same', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7569a",
   "metadata": {},
   "source": [
    "find the indicator for gas price based on the previous price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76546112",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_avg_price = pd.DataFrame(oil_dict.items(), columns = ['years', 'avg_oil_price'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6cf2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_avg_price.years = year_avg_price.years.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac86175",
   "metadata": {},
   "source": [
    "getting the mean of gas price of each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805aad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(year_avg_price,\n",
    "               left_on= 'date_year',\n",
    "               right_on= 'years')\n",
    "\n",
    "test_df = test_df.merge(year_avg_price,\n",
    "               left_on= 'date_year',\n",
    "               right_on= 'years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'is_higher_than_avg_oil_price'] = train_df.apply(lambda row: 1 if row['dcoilwtico'] > row['avg_oil_price'] else 0, axis = 1)\n",
    "test_df.loc[:, 'is_higher_than_avg_oil_price'] = test_df.apply(lambda row: 1 if row['dcoilwtico'] > row['avg_oil_price'] else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4825a",
   "metadata": {},
   "source": [
    "- seprate the years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e102477",
   "metadata": {},
   "source": [
    "Based on the plot above, it looks like the price before 2015 and after 2015 would have different patterns. Therefore, let's seperate these price gap so that the model can distinguish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d607cf2",
   "metadata": {},
   "source": [
    "find the difference for month to month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48857553",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_total_avg = np.mean(oil_price.dcoilwtico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price_avg_by_month = train_df.groupby('year_month').mean()[['dcoilwtico']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price_avg_by_month[oil_price_avg_by_month.dcoilwtico < oil_total_avg]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6ef2b",
   "metadata": {},
   "source": [
    "Looks like after 2014-12, the price of oil dropped dramatically. Let's divide in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns= {'date_x':'date'}, inplace = True)\n",
    "test_df.rename(columns= {'date_x':'date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns= ['date_y', 'date_join'], inplace = True)\n",
    "test_df.drop(columns= ['date_y', 'date_join'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'is_after_2014-12'] = train_df.date.apply(lambda x: 1 if x >= pd.to_datetime('2014-12-01') else 0)\n",
    "test_df.loc[:, 'is_after_2014-12'] = test_df.date.apply(lambda x: 1 if x >= pd.to_datetime('2014-12-01') else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3755c4",
   "metadata": {},
   "source": [
    "###### Holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd84dd",
   "metadata": {},
   "source": [
    "- holiday missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c66b86",
   "metadata": {},
   "source": [
    "There are some missing values in holiday related features. This is because\n",
    "\n",
    "1. Null values in holiday_counts happen because the given dates are not holidays. Therefore, we can impute those missing values as 0.\n",
    "\n",
    "2. Null values in is_multiple also happen for the same reason. We can also impute those values as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.holiday_counts.fillna(0, inplace = True)\n",
    "test_df.holiday_counts.fillna(0, inplace = True)\n",
    "\n",
    "train_df.is_multiple.fillna(0, inplace = True)\n",
    "test_df.is_multiple.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b78eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot.holiday_counts.fillna(0, inplace = True)\n",
    "test_df.holiday_counts.fillna(0, inplace = True)\n",
    "\n",
    "train_plot.is_multiple.fillna(0, inplace = True)\n",
    "test_df.is_multiple.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot.loc[:, 'mults'] = train_plot.is_multiple.apply(lambda x: 'multiple' if x == 1 else 'N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8fb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "plt.title(\"Sales: Multiple Holidays vs Regular Days\", fontsize = 18)\n",
    "sns.barplot(x = 'mults',\n",
    "            color = 'blue',\n",
    "            alpha = 0.7,\n",
    "            y = 'sales',\n",
    "            data = train_plot)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.axhline(np.mean(train_df['sales']), color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ce283",
   "metadata": {},
   "source": [
    "- transferred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007113e",
   "metadata": {},
   "source": [
    "using the function we create above, let's find out the different time frames and the sales difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_holiday = pd.read_pickle('../asset/transfer_holidays.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(transfer_holiday[['date', 'transferred']], left_on= 'date', right_on= 'date', how = 'left')\n",
    "test_df = test_df.merge(transfer_holiday[['date', 'transferred']], left_on= 'date', right_on= 'date', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c002a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transferred = find_the_time_range(train_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = compare_the_means(transferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_dict = {}\n",
    "\n",
    "for val in pvalues.values():\n",
    "    for key in val:\n",
    "        if key not in delta_dict:\n",
    "            delta_dict[key] = 1\n",
    "        else:\n",
    "            delta_dict[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_delta = dict(sorted(delta_dict.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a272ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(sorted_delta.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_delta = {}\n",
    "\n",
    "for k, v in delta_dict.items():\n",
    "    if v > 6:\n",
    "        high_delta[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b2da6",
   "metadata": {},
   "source": [
    "let's use this information for the transferred holidays. meaning if transferred == True, then -,+ 3 and 4 will have a higher sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fa8d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "delta_list = [-4, -3, 3, 4]\n",
    "\n",
    "train_df = find_delta_date(train_df, delta_list)\n",
    "test_df = find_delta_date(test_df, delta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46871244",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = compare_dates_for_delta(train_df)\n",
    "test_df = compare_dates_for_delta(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21417a5e",
   "metadata": {},
   "source": [
    "- christmas sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_season = ['2013-12', '2014-12', '2015-12', '2016-12']\n",
    "\n",
    "for date in christmas_season:\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.title(f\"Christmas Sales in {date}\", fontsize = 15)\n",
    "    sns.lineplot(x = 'date',\n",
    "                 y = 'sales',\n",
    "                 data = train_df[train_df.year_month == date] )\n",
    "    plt.xticks(fontsize = 15, rotation = 270)\n",
    "    plt.yticks(fontsize = 15)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.axhline(np.mean(train_df[train_df.year_month == date]['sales']), color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333a5bb",
   "metadata": {},
   "source": [
    "Looks like around Christmas seasons, there is an increase in terms of the total sales. We can look into dates before Christmas since the stores are closed on the day of Christmas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c86f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "christmas_list = ['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6dc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_christmas = find_christmas_sales(christmas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17f808",
   "metadata": {},
   "source": [
    "Using the function above, we can find the range of Christmas season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd218e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'christmas_sales_season'] = train_df.date.apply(lambda x: 1 if x in total_christmas else 0)\n",
    "test_df.loc[:, 'christmas_sales_season'] = test_df.date.apply(lambda x: 1 if x in total_christmas else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cca050",
   "metadata": {},
   "source": [
    "Assign 1 if those dates are within the range of Christmas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3459f",
   "metadata": {},
   "source": [
    "###### city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77aa6a5",
   "metadata": {},
   "source": [
    "both city and states are geolocation information and might deliver similar information. let's use average to determine if we can distinguish between low and high sales regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebabc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.groupby(['state','city']).mean()[['sales']].sort_values(by = 'sales', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74f5b7",
   "metadata": {},
   "source": [
    "based on the table above, looks like we can only use state information rather than city since the average is consistent throughout the region. in addition to this, there are not many cities under each state, so we can use state instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c0a2b",
   "metadata": {},
   "source": [
    "###### state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43db87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sales_summary = train_df.groupby(['state']).mean()[['sales']].sort_values(by = 'sales', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68485ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sales_1st = state_sales_summary.describe().T['25%'].values[0]\n",
    "state_sales_3rd = state_sales_summary.describe().T['75%'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26262ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sales_summary.loc[:, 'state_sales_cut'] = state_sales_summary.sales.apply(lambda x: 'low' if x < state_sales_1st else ('med' if x < state_sales_3rd else 'high'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sales_summary = state_sales_summary.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(state_sales_summary[['state', 'state_sales_cut']], \n",
    "               left_on = 'state',\n",
    "               right_on = 'state',\n",
    "               how = 'left')\n",
    "\n",
    "\n",
    "test_df = test_df.merge(state_sales_summary[['state', 'state_sales_cut']], \n",
    "               left_on = 'state',\n",
    "               right_on = 'state',\n",
    "               how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0dab20",
   "metadata": {},
   "source": [
    "###### store_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_type_summary = train_df.groupby('type').mean()[['sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_type_summary.sort_values(by = 'sales', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 8))\n",
    "plt.title('Sales Based on Store Types', fontsize = 20)\n",
    "store_avg_bar = sns.barplot(x = store_type_summary.index, \n",
    "            y = 'sales',\n",
    "            color = 'blue',\n",
    "            alpha = 0.6,\n",
    "            data = store_type_summary)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "store_avg_bar.axhline(np.mean(store_type_summary)['sales'], linewidth = 3, color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa433bf",
   "metadata": {},
   "source": [
    "looks like variation is high for the store type A, so we can encode the store type A as high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:, 'store_type_sales'] = train_df.type.apply(lambda x: 'high' if x == 'A' else 'low')\n",
    "test_df.loc[:, 'store_type_sales'] = test_df.type.apply(lambda x: 'high' if x == 'A' else 'low')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f4ee6",
   "metadata": {},
   "source": [
    "###### cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5975401",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sales = train_df.groupby('cluster').mean()[['sales']].sort_values(by = 'sales', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e099177",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sales_avg = cluster_sales.describe().T['mean'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sales.loc[:, 'cluster_sales_indicator'] = cluster_sales.sales.apply(lambda x: 'higher_than_avg' if x > cluster_sales_avg else 'lower_than_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9776935",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sales.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 8))\n",
    "plt.title('Sales Based on Clusters', fontsize = 20)\n",
    "store_avg_bar = sns.barplot(x = 'cluster', \n",
    "            y = 'sales',\n",
    "            color = 'blue',\n",
    "            alpha = 0.6,\n",
    "            data = cluster_sales)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "store_avg_bar.axhline(np.mean(cluster_sales)['sales'], linewidth = 3, color = 'red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf887cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(cluster_sales[['cluster', 'cluster_sales_indicator']], \n",
    "               left_on = 'cluster',\n",
    "               right_on = 'cluster',\n",
    "               how = 'left')\n",
    "\n",
    "\n",
    "test_df = test_df.merge(cluster_sales[['cluster', 'cluster_sales_indicator']], \n",
    "               left_on = 'cluster',\n",
    "               right_on = 'cluster',\n",
    "               how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812fbd0",
   "metadata": {},
   "source": [
    "### Export the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7df1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = '../asset/'\n",
    "\n",
    "with open(export_path + 'train_df.pkl', 'wb') as f:\n",
    "    pickle.dump(train_df, f)\n",
    "    \n",
    "with open(export_path + 'test_df.pkl', 'wb') as f:\n",
    "    pickle.dump(test_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
