{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfca37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fbprophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d3f53",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282fd645",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../asset/train_df.pkl')\n",
    "test_df = pd.read_pickle('../asset/test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9309c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.store_nbr.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73718504",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.family.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67256629",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276c3d8",
   "metadata": {},
   "source": [
    "The original datasets have 54 unique stores and 33 unique departments, which are the total of 1782 unique combibations. This might deliver the detailed information regarding departments, but it takes a long time for us to train the model. What if we combine the total sales per stores and get the erros from there? Would there be a huge difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_group_by = train_df.groupby(['date', 'store_nbr']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b400b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_group_by = train_df_group_by[['date', 'store_nbr', 'sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_group_by.rename(columns= {'sales':'total_sales'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da050388",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_combined = pd.merge(train_df, train_df_group_by,\n",
    "                             left_on= ['date', 'store_nbr'],\n",
    "                             right_on= ['date', 'store_nbr'],\n",
    "                             how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df_combined) == len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a789db9",
   "metadata": {},
   "source": [
    "Looks like join worked fine. There was no duplicate issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = train_df_combined.drop(columns= ['family', 'sales']).drop_duplicates(['date', 'store_nbr', 'total_sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.date.nunique() *train_df.store_nbr.nunique() == len(final_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f18bed",
   "metadata": {},
   "source": [
    "Drop rows if they are duplicates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3c140",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_features(data):\n",
    "    \"\"\"This function will help us to convert boolean and cateogrical values to numerical values.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    \n",
    "    Output:\n",
    "    final_df -> dataframe (after feature conversions including one hot encoding and ordinal encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting features that we will use for the Prophet model\n",
    "    features = ['date', 'is_weekend', 'holiday_counts', 'is_multiple', 'is_above_median','store_sales_bins', \n",
    "                'family_sales_bins', 'onpromo_avg_bins', 'price_indicator', 'is_higher_than_avg_oil_price',\n",
    "                'is_delta_-4', 'is_delta_-3', 'is_delta_3', 'is_delta_4', 'is_after_2014-12', \n",
    "                'state_sales_cut','store_type_sales', 'cluster_sales_indicator', 'christmas_sales_season']\n",
    "    \n",
    "    # treat train and test dataset in a different way\n",
    "    if 'total_sales' in data.columns:\n",
    "        features.append('total_sales')\n",
    "        \n",
    "    new_data = data[features]\n",
    "    \n",
    "    oe = OrdinalEncoder()\n",
    "    \n",
    "    # based on the features, use different methods for encoders. \n",
    "    for col, dtype in new_data.dtypes.items():\n",
    "        if col in ['id', 'date']:\n",
    "            pass\n",
    "        elif dtype == 'bool':\n",
    "            new_data.loc[:, col] = new_data[col].apply(lambda x: int(x))\n",
    "        elif col in ['store_sales_bins', 'family_sales_bins', 'onpromo_avg_bins', 'cluster_sales_indicator']:\n",
    "            new_data.loc[:, col] = oe.fit_transform(new_data[col].values.reshape(-1,1))\n",
    "    \n",
    "    # for categorical values, create dummy varaibles\n",
    "    # for train, we should include sales\n",
    "    # for test, use the dataframe before joining\n",
    "    \n",
    "    final_df = pd.get_dummies(new_data)\n",
    "#     if 'sales' not in new_data.columns:\n",
    "#         final_df = pd.get_dummies(new_data)\n",
    "#     else:\n",
    "#         new_data = pd.get_dummies(new_data)\n",
    "#         final_df = pd.concat([new_data, new_data['sales']], axis = 1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c93f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split(data, train_size):\n",
    "    \"\"\"This function will help us create train test split in time series.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    train_size -> float (the percentage of train_set in our dataset)\n",
    "    \n",
    "    Output:\n",
    "    train_df -> dataframe (train_set based on the train_size)\n",
    "    test_df -> dataframe (test_set based on the train_size)\"\"\"\n",
    "    \n",
    "    total_row = len(data)\n",
    "    train_idx = int(total_row * train_size)\n",
    "    \n",
    "    train = data[:train_idx]\n",
    "    test = data[train_idx:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prophet(prophet_df, ci):\n",
    "    \n",
    "    \"\"\"This function will help us build forecasting models using Prophet. \n",
    "    Since our dataset is based on store numbers, we can create different models based on stores.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe that contains store information, geo location, and various features)\n",
    "    \n",
    "    Output:\n",
    "    prophet_dict -> dict (key: store, value: yhat and y (dataframe))\"\"\"\n",
    "    \n",
    "    # create a dictionary to save information\n",
    "    prophet_dict = {}\n",
    "    christmas_df = pd.DataFrame(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], columns = ['ds'])\n",
    "    \n",
    "    # create a unique stores list\n",
    "    unique_store = list(prophet_df.store_nbr.unique())\n",
    "    \n",
    "    for key in unique_store:\n",
    "\n",
    "        data = prophet_df[prophet_df.store_nbr == key]\n",
    "        data = data.reset_index()\n",
    "        data.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "        data_converted = convert_features(data)\n",
    "        data_converted.rename(columns= {'date':'ds', 'total_sales':'y'}, inplace = True)\n",
    "\n",
    "        # time series split (70:30)\n",
    "        train, test = time_split(data_converted, 0.7)\n",
    "        train.drop_duplicates(inplace = True)\n",
    "        test.drop_duplicates(inplace = True)\n",
    "        test_len = len(test)\n",
    "\n",
    "        model = Prophet(interval_width= ci)\n",
    "\n",
    "        # adding regressors for the Prophet model\n",
    "        features = list(train.columns)\n",
    "\n",
    "        for feature in features:\n",
    "            if feature not in ['ds', 'y', 'unique_store_dep_key']:\n",
    "                model.add_regressor(feature, standardize = True)\n",
    "\n",
    "        model.fit(train)\n",
    "\n",
    "        # create a future dataframe so that we can compare the y and yhat\n",
    "        future = model.make_future_dataframe(periods= test_len - 1)\n",
    "\n",
    "        # Christmas is not added so we should manually add them\n",
    "        future.append(christmas_df, ignore_index = True)\n",
    "\n",
    "        # combine two data sources together\n",
    "        origial = pd.concat([train, test])\n",
    "\n",
    "        # add features\n",
    "        new_future = pd.merge(future, origial,\n",
    "                              left_on= 'ds',\n",
    "                              right_on= 'ds',\n",
    "                              how = 'left')\n",
    "\n",
    "        # create a forecastibng for the validation set\n",
    "        forecast = model.predict(new_future)\n",
    "\n",
    "        # select features\n",
    "        selected_forecast = forecast[['ds', 'yhat_lower', 'yhat_upper', 'yhat']]\n",
    "\n",
    "        # save the results in the dictionary\n",
    "        prophet_dict[key] = selected_forecast\n",
    "        \n",
    "    return prophet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d405515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_y_yhat(original_df, pred_df):\n",
    "    \n",
    "    \"\"\"This function will return a combination of yhat from the Prophet model\n",
    "       and the actual y from the original dataset.\n",
    "       \n",
    "       Input:\n",
    "       \n",
    "       original_df -> dataframe (contain information regarding unique_key, date, and actual)\n",
    "       pred_df -> dataframe (contain information regarding the yhat)\n",
    "       \n",
    "       Output:\n",
    "       \n",
    "       combined_dict -> dictionary (key: unique_key, value: contain information from the both datasets)\"\"\"\n",
    "    \n",
    "    # create a dictionary to save the results\n",
    "    combined_dict = {}\n",
    "    \n",
    "    # create a unique stores list\n",
    "    unique_store = list(prophet_df.store_nbr.unique())\n",
    "    \n",
    "    for key in unique_store:\n",
    "        # original dataset\n",
    "        true_df = original_df[original_df.store_nbr == key]\n",
    "        # yhat dataset from the Prophet model\n",
    "        yhat_df = pred_df[key]\n",
    "        \n",
    "        filtered_true = true_df[['date', 'total_sales']]\n",
    "        filtered_true.rename(columns = {'date':'ds'}, inplace = True)\n",
    "        \n",
    "        # create lists to distinguish the difference between train and test\n",
    "        total_len = len(filtered_true)\n",
    "        train_idx = int(total_len * 0.7)\n",
    "        test_idx = total_len - train_idx\n",
    "        \n",
    "        # create indicators based on the length of data -> train and test\n",
    "        train_ind_list = ['train' for _ in range(train_idx)]\n",
    "        test_ind_list = ['test' for _ in range(test_idx)]\n",
    "        combined_ind_list = train_ind_list + test_ind_list\n",
    "        \n",
    "        # assign a column so that we can distinguish them\n",
    "        filtered_true.loc[:, 'indicator'] = combined_ind_list\n",
    "        \n",
    "        \n",
    "        merged_df = pd.merge(filtered_true, yhat_df,\n",
    "                             left_on = 'ds',\n",
    "                             right_on = 'ds')\n",
    "        merged_df.rename(columns = {'total_sales':'y'}, inplace = True)\n",
    "        \n",
    "        # save information in the dictionary\n",
    "        combined_dict[key] = merged_df\n",
    "        \n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mae(pred_dict):\n",
    "    \n",
    "    \"\"\"This function will find the rmse for each key within the dictionary that we created using the Prophet model.\n",
    "    \n",
    "    Input:\n",
    "    pred_dict -> dict (key: unique_key (store number and department), value: dataframe (y hat and y))\n",
    "    \n",
    "    Output:\n",
    "    error_df -> dict (dataframe that contains information regarding unique key and mean absolute error)\n",
    "    \"\"\"\n",
    "    \n",
    "    error_dict = {}\n",
    "    \n",
    "    for key in pred_dict:\n",
    "        \n",
    "        # start looping through the dictionary \n",
    "        data = pred_dict[key]\n",
    "        data.loc[:, 'mean_abs_error'] = np.abs(data['yhat'] - data['y'])\n",
    "        \n",
    "        # check errors between train and test\n",
    "        errors_partition = data.groupby(['indicator']).sum()[['mean_abs_error']]\n",
    "        errors_by_group = errors_partition.reset_index()\n",
    "        \n",
    "        train_error = errors_by_group[errors_by_group.indicator == 'train']['mean_abs_error'].values[0]\n",
    "        test_error = errors_by_group[errors_by_group.indicator == 'test']['mean_abs_error'].values[0]\n",
    "        \n",
    "        # save the results in the dictionary\n",
    "        error_dict[key] = {'train_error':train_error,\n",
    "                           'test_error':test_error}\n",
    "        \n",
    "        \n",
    "    return error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prophet_remove_zero(prophet_df, exception, ci):\n",
    "    \n",
    "    \"\"\"This function will help us build forecasting models using Prophet. \n",
    "    Since our dataset is based on store numbers, we can create different models based on stores.\n",
    "    This function does not include store-department unique key in the model if they have 0 sales. \n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe that contains store information, geo location, and various features)\n",
    "    \n",
    "    Output:\n",
    "    prophet_dict -> dict (key: store, value: yhat and y (dataframe))\"\"\"\n",
    "    \n",
    "    # create a dictionary to save information\n",
    "    prophet_dict = {}\n",
    "    christmas_df = pd.DataFrame(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], columns = ['ds'])\n",
    "    \n",
    "    # create a unique stores list\n",
    "    unique_store = list(prophet_df.store_nbr.unique())\n",
    "    \n",
    "    \n",
    "    for key in unique_store:\n",
    "        \n",
    "        # if the unique_key is in the zero sales list, then we are not taking them into a consideration\n",
    "        if key in exception:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            data = prophet_df[prophet_df.unique_store_dep_key == key]\n",
    "            data = data.reset_index()\n",
    "            data.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "            data_converted = convert_features(data)\n",
    "            data_converted.rename(columns= {'date':'ds', 'total_sales':'y'}, inplace = True)\n",
    "\n",
    "            # time series split (70:30)\n",
    "            train, test = time_split(data_converted, 0.7)\n",
    "            train.drop_duplicates(inplace = True)\n",
    "            test.drop_duplicates(inplace = True)\n",
    "            test_len = len(test)\n",
    "\n",
    "            model = Prophet(interval_width= ci)\n",
    "\n",
    "            # adding regressors for the Prophet model\n",
    "            features = list(train.columns)\n",
    "\n",
    "            for feature in features:\n",
    "                if feature not in ['ds', 'y', 'unique_store_dep_key']:\n",
    "                    model.add_regressor(feature, standardize = True)\n",
    "\n",
    "            model.fit(train)\n",
    "\n",
    "            # create a future dataframe so that we can compare the y and yhat\n",
    "            future = model.make_future_dataframe(periods= test_len - 1)\n",
    "\n",
    "            # Christmas is not added so we should manually add them\n",
    "            future.append(christmas_df, ignore_index = True)\n",
    "\n",
    "            # combine two data sources together\n",
    "            origial = pd.concat([train, test])\n",
    "\n",
    "            # add features\n",
    "            new_future = pd.merge(future, origial,\n",
    "                                  left_on= 'ds',\n",
    "                                  right_on= 'ds',\n",
    "                                  how = 'left')\n",
    "\n",
    "            # create a forecastibng for the validation set\n",
    "            forecast = model.predict(new_future)\n",
    "\n",
    "            # select features\n",
    "            selected_forecast = forecast[['ds', 'yhat_lower', 'yhat_upper', 'yhat']]\n",
    "\n",
    "            # save the results in the dictionary\n",
    "            prophet_dict[key] = selected_forecast\n",
    "        \n",
    "    return prophet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_errors(pred_dict, actual, yhat):\n",
    "    \n",
    "    \"\"\"This function will calculate the erorrs using mean absolute erorrs.\n",
    "    \n",
    "    Input:\n",
    "    pred_dict -> dict (key: unique_key, values: y and yhat)\n",
    "    actual -> (values: the actual y value)\n",
    "    yhat -> (values: the predicdted value from the model)\n",
    "    \n",
    "    Output:\n",
    "    error -> int (mean absolute erorr)\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    error_total = 0\n",
    "    # looping through the dictionary and summing the error.\n",
    "    for key in pred_dict:\n",
    "        error = mean_absolute_error(pred_dict[key][actual], pred_dict[key][yhat])\n",
    "        error_total += error\n",
    "        \n",
    "    return error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c83b71",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52686777",
   "metadata": {},
   "source": [
    "- Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f431d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prophet_pred = create_prophet(final_train_df, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame()\n",
    "\n",
    "for key in prophet_pred:\n",
    "    temp = prophet_pred[key]\n",
    "    temp.loc[:, 'store_nbr'] = key\n",
    "    \n",
    "    # create lists to distinguish the difference between train and test\n",
    "    total_len = len(temp)\n",
    "    train_idx = int(total_len * 0.7)\n",
    "    test_idx = total_len - train_idx\n",
    "\n",
    "    # create indicators based on the length of data -> train and test\n",
    "    train_ind_list = ['train' for _ in range(train_idx)]\n",
    "    test_ind_list = ['test' for _ in range(test_idx)]\n",
    "    combined_ind_list = train_ind_list + test_ind_list\n",
    "    combined = pd.merge(temp, final_train_df[['date', 'total_sales', 'store_nbr']],\n",
    "                        left_on= ['ds', 'store_nbr'],\n",
    "                        right_on= ['date', 'store_nbr'],\n",
    "                        how = 'inner')\n",
    "    combined.loc[:, 'indicator'] = combined_ind_list\n",
    "    total_df = total_df.append(combined, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88737b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc[:, 'mae'] = np.abs(total_df['yhat'] - total_df['total_sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eaf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_by_stores = total_df.groupby(['store_nbr', 'indicator']).sum()[['mae']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43589ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = errors_by_stores[errors_by_stores.indicator == 'train']\n",
    "test_error = errors_by_stores[errors_by_stores.indicator == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd309aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_sum = round(np.sum(train_error.mae), 4)\n",
    "test_error_sum = round(np.sum(test_error.mae), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using Prophet, the mean absolute error in the training dataset is {train_error_sum}\")\n",
    "print(f\"Using Prophet, the mean absolute error in the validation dataset is {test_error_sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18a309",
   "metadata": {},
   "source": [
    "Looks like training is a lot quicker than the individual models, but the difference in errors is dramatically huge. We have to stick with the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb95cf",
   "metadata": {},
   "source": [
    "### Export the aggregated df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcfabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/aggregated_train_df.pkl', 'wb') as f:\n",
    "    pickle.dump(final_train_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4cf782",
   "metadata": {},
   "source": [
    "Let's try to use the same dataframe on XGBoost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
