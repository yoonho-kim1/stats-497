{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfca37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fbprophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d3f53",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282fd645",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../asset/train_df.pkl')\n",
    "test_df = pd.read_pickle('../asset/test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3c140",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_features(data):\n",
    "    \"\"\"This function will help us to convert boolean and cateogrical values to numerical values.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    \n",
    "    Output:\n",
    "    final_df -> dataframe (after feature conversions including one hot encoding and ordinal encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting features that we will use for the Prophet model\n",
    "    features = ['date', 'is_weekend', 'holiday_counts', 'is_multiple', 'is_above_median','store_sales_bins', \n",
    "                'family_sales_bins', 'onpromo_avg_bins', 'price_indicator', 'is_higher_than_avg_oil_price',\n",
    "                'is_delta_-4', 'is_delta_-3', 'is_delta_3', 'is_delta_4', 'is_after_2014-12', \n",
    "                'state_sales_cut','store_type_sales', 'cluster_sales_indicator', 'christmas_sales_season']\n",
    "    \n",
    "    # treat train and test dataset in a different way\n",
    "    if 'sales' in data.columns:\n",
    "        features.append('sales')\n",
    "        \n",
    "    new_data = data[features]\n",
    "    \n",
    "    oe = OrdinalEncoder()\n",
    "    \n",
    "    # based on the features, use different methods for encoders. \n",
    "    for col, dtype in new_data.dtypes.items():\n",
    "        if col in ['id', 'date']:\n",
    "            pass\n",
    "        elif dtype == 'bool':\n",
    "            new_data.loc[:, col] = new_data[col].apply(lambda x: int(x))\n",
    "        elif col in ['store_sales_bins', 'family_sales_bins', 'onpromo_avg_bins', 'cluster_sales_indicator']:\n",
    "            new_data.loc[:, col] = oe.fit_transform(new_data[col].values.reshape(-1,1))\n",
    "    \n",
    "    # for categorical values, create dummy varaibles\n",
    "    # for train, we should include sales\n",
    "    # for test, use the dataframe before joining\n",
    "    \n",
    "    final_df = pd.get_dummies(new_data)\n",
    "#     if 'sales' not in new_data.columns:\n",
    "#         final_df = pd.get_dummies(new_data)\n",
    "#     else:\n",
    "#         new_data = pd.get_dummies(new_data)\n",
    "#         final_df = pd.concat([new_data, new_data['sales']], axis = 1)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c93f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split(data, train_size):\n",
    "    \"\"\"This function will help us create train test split in time series.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    train_size -> float (the percentage of train_set in our dataset)\n",
    "    \n",
    "    Output:\n",
    "    train_df -> dataframe (train_set based on the train_size)\n",
    "    test_df -> dataframe (test_set based on the train_size)\"\"\"\n",
    "    \n",
    "    total_row = len(data)\n",
    "    train_idx = int(total_row * train_size)\n",
    "    \n",
    "    train = data[:train_idx]\n",
    "    test = data[train_idx:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prophet(prophet_df, ci):\n",
    "    \n",
    "    \"\"\"This function will help us build forecasting models using Prophet. \n",
    "    Since our dataset is based on store numbers, we can create different models based on stores.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe that contains store information, geo location, and various features)\n",
    "    \n",
    "    Output:\n",
    "    prophet_dict -> dict (key: store, value: yhat and y (dataframe))\"\"\"\n",
    "    \n",
    "    # create a dictionary to save information\n",
    "    prophet_dict = {}\n",
    "    christmas_df = pd.DataFrame(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], columns = ['ds'])\n",
    "    \n",
    "    # create a unique_key to create individual models. \n",
    "    prophet_df.loc[:, 'unique_store_dep_key'] = prophet_df.store_nbr.apply(lambda x: str(x)) + '-' + prophet_df.family.apply(lambda x: str(x))\n",
    "    unique_keys = list(prophet_df.unique_store_dep_key.unique())\n",
    "    \n",
    "    for key in unique_keys:\n",
    "\n",
    "        data = prophet_df[prophet_df.unique_store_dep_key == key]\n",
    "        data = data.reset_index()\n",
    "        data.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "        data_converted = convert_features(data)\n",
    "        data_converted.rename(columns= {'date':'ds', 'sales':'y'}, inplace = True)\n",
    "\n",
    "        # time series split (70:30)\n",
    "        train, test = time_split(data_converted, 0.7)\n",
    "        train.drop_duplicates(inplace = True)\n",
    "        test.drop_duplicates(inplace = True)\n",
    "        test_len = len(test)\n",
    "\n",
    "        model = Prophet(interval_width= ci)\n",
    "\n",
    "        # adding regressors for the Prophet model\n",
    "        features = list(train.columns)\n",
    "\n",
    "        for feature in features:\n",
    "            if feature not in ['ds', 'y', 'unique_store_dep_key']:\n",
    "                model.add_regressor(feature, standardize = True)\n",
    "\n",
    "        model.fit(train)\n",
    "\n",
    "        # add date features\n",
    "        features.append(['ds', 'unique_store_dep_key'])\n",
    "\n",
    "\n",
    "        # create a future dataframe so that we can compare the y and yhat\n",
    "        future = model.make_future_dataframe(periods= test_len - 1)\n",
    "\n",
    "        # Christmas is not added so we should manually add them\n",
    "        future.append(christmas_df, ignore_index = True)\n",
    "\n",
    "        # combine two data sources together\n",
    "        origial = pd.concat([train, test])\n",
    "\n",
    "        # add features\n",
    "        new_future = pd.merge(future, origial,\n",
    "                              left_on= 'ds',\n",
    "                              right_on= 'ds',\n",
    "                              how = 'left')\n",
    "\n",
    "        # create a forecastibng for the validation set\n",
    "        forecast = model.predict(new_future)\n",
    "\n",
    "        # select features\n",
    "        selected_forecast = forecast[['ds', 'yhat_lower', 'yhat_upper', 'yhat']]\n",
    "\n",
    "        # save the results in the dictionary\n",
    "        prophet_dict[key] = selected_forecast\n",
    "        \n",
    "    return prophet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d405515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_y_yhat(original_df, pred_df):\n",
    "    \n",
    "    \"\"\"This function will return a combination of yhat from the Prophet model\n",
    "       and the actual y from the original dataset.\n",
    "       \n",
    "       Input:\n",
    "       \n",
    "       original_df -> dataframe (contain information regarding unique_key, date, and actual)\n",
    "       pred_df -> dataframe (contain information regarding the yhat)\n",
    "       \n",
    "       Output:\n",
    "       \n",
    "       combined_dict -> dictionary (key: unique_key, value: contain information from the both datasets)\"\"\"\n",
    "    \n",
    "    # create a dictionary to save the results\n",
    "    combined_dict = {}\n",
    "    \n",
    "    # create a unique key for the individual stores and their departments\n",
    "    original_df.loc[:, 'unique_store_dep_key'] = original_df.store_nbr.apply(lambda x: str(x)) + '-' + original_df.family.apply(lambda x: str(x))\n",
    "    unique_keys = list(pred_df.keys())\n",
    "    \n",
    "    \n",
    "    for key in unique_keys:\n",
    "        # original dataset\n",
    "        true_df = original_df[original_df.unique_store_dep_key == key]\n",
    "        # yhat dataset from the Prophet model\n",
    "        yhat_df = pred_df[key]\n",
    "        \n",
    "        filtered_true = true_df[['date', 'sales']]\n",
    "        filtered_true.rename(columns = {'date':'ds'}, inplace = True)\n",
    "        \n",
    "        # create lists to distinguish the difference between train and test\n",
    "        total_len = len(filtered_true)\n",
    "        train_idx = int(total_len * 0.7)\n",
    "        test_idx = total_len - train_idx\n",
    "        \n",
    "        # create indicators based on the length of data -> train and test\n",
    "        train_ind_list = ['train' for _ in range(train_idx)]\n",
    "        test_ind_list = ['test' for _ in range(test_idx)]\n",
    "        combined_ind_list = train_ind_list + test_ind_list\n",
    "        \n",
    "        # assign a column so that we can distinguish them\n",
    "        filtered_true.loc[:, 'indicator'] = combined_ind_list\n",
    "        \n",
    "        \n",
    "        merged_df = pd.merge(filtered_true, yhat_df,\n",
    "                             left_on = 'ds',\n",
    "                             right_on = 'ds')\n",
    "        merged_df.rename(columns = {'sales':'y'}, inplace = True)\n",
    "        \n",
    "        # save information in the dictionary\n",
    "        combined_dict[key] = merged_df\n",
    "        \n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mae(pred_dict):\n",
    "    \n",
    "    \"\"\"This function will find the rmse for each key within the dictionary that we created using the Prophet model.\n",
    "    \n",
    "    Input:\n",
    "    pred_dict -> dict (key: unique_key (store number and department), value: dataframe (y hat and y))\n",
    "    \n",
    "    Output:\n",
    "    error_df -> dict (dataframe that contains information regarding unique key and mean absolute error)\n",
    "    \"\"\"\n",
    "    \n",
    "    error_dict = {}\n",
    "    \n",
    "    for key in pred_dict:\n",
    "        \n",
    "        # start looping through the dictionary \n",
    "        data = pred_dict[key]\n",
    "        data.loc[:, 'mean_abs_error'] = np.abs(data['yhat'] - data['y'])\n",
    "        \n",
    "        # check errors between train and test\n",
    "        errors_partition = data.groupby(['indicator']).sum()[['mean_abs_error']]\n",
    "        errors_by_group = errors_partition.reset_index()\n",
    "        \n",
    "        train_error = errors_by_group[errors_by_group.indicator == 'train']['mean_abs_error'].values[0]\n",
    "        test_error = errors_by_group[errors_by_group.indicator == 'test']['mean_abs_error'].values[0]\n",
    "        \n",
    "        # save the results in the dictionary\n",
    "        error_dict[key] = {'train_error':train_error,\n",
    "                           'test_error':test_error}\n",
    "        \n",
    "        \n",
    "    return error_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prophet_remove_zero(prophet_df, exception, ci):\n",
    "    \n",
    "    \"\"\"This function will help us build forecasting models using Prophet. \n",
    "    Since our dataset is based on store numbers, we can create different models based on stores.\n",
    "    This function does not include store-department unique key in the model if they have 0 sales. \n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe that contains store information, geo location, and various features)\n",
    "    \n",
    "    Output:\n",
    "    prophet_dict -> dict (key: store, value: yhat and y (dataframe))\"\"\"\n",
    "    \n",
    "    # create a dictionary to save information\n",
    "    prophet_dict = {}\n",
    "    christmas_df = pd.DataFrame(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], columns = ['ds'])\n",
    "    \n",
    "    # create a unique_key to create individual models. \n",
    "    prophet_df.loc[:, 'unique_store_dep_key'] = prophet_df.store_nbr.apply(lambda x: str(x)) + '-' + prophet_df.family.apply(lambda x: str(x))\n",
    "    unique_keys = list(prophet_df.unique_store_dep_key.unique())\n",
    "    \n",
    "    for key in unique_keys:\n",
    "        \n",
    "        # if the unique_key is in the zero sales list, then we are not taking them into a consideration\n",
    "        if key in exception:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            data = prophet_df[prophet_df.unique_store_dep_key == key]\n",
    "            data = data.reset_index()\n",
    "            data.drop(columns = ['index'], inplace = True)\n",
    "\n",
    "            data_converted = convert_features(data)\n",
    "            data_converted.rename(columns= {'date':'ds', 'sales':'y'}, inplace = True)\n",
    "\n",
    "            # time series split (70:30)\n",
    "            train, test = time_split(data_converted, 0.7)\n",
    "            train.drop_duplicates(inplace = True)\n",
    "            test.drop_duplicates(inplace = True)\n",
    "            test_len = len(test)\n",
    "\n",
    "            model = Prophet(interval_width= ci)\n",
    "\n",
    "            # adding regressors for the Prophet model\n",
    "            features = list(train.columns)\n",
    "\n",
    "            for feature in features:\n",
    "                if feature not in ['ds', 'y', 'unique_store_dep_key']:\n",
    "                    model.add_regressor(feature, standardize = True)\n",
    "\n",
    "            model.fit(train)\n",
    "\n",
    "            # add date features\n",
    "            features.append(['ds', 'unique_store_dep_key'])\n",
    "\n",
    "\n",
    "            # create a future dataframe so that we can compare the y and yhat\n",
    "            future = model.make_future_dataframe(periods= test_len - 1)\n",
    "\n",
    "            # Christmas is not added so we should manually add them\n",
    "            future.append(christmas_df, ignore_index = True)\n",
    "\n",
    "            # combine two data sources together\n",
    "            origial = pd.concat([train, test])\n",
    "\n",
    "            # add features\n",
    "            new_future = pd.merge(future, origial,\n",
    "                                  left_on= 'ds',\n",
    "                                  right_on= 'ds',\n",
    "                                  how = 'left')\n",
    "\n",
    "            # create a forecastibng for the validation set\n",
    "            forecast = model.predict(new_future)\n",
    "\n",
    "            # select features\n",
    "            selected_forecast = forecast[['ds', 'yhat_lower', 'yhat_upper', 'yhat']]\n",
    "\n",
    "            # save the results in the dictionary\n",
    "            prophet_dict[key] = selected_forecast\n",
    "        \n",
    "    return prophet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_errors(pred_dict, actual, yhat):\n",
    "    \n",
    "    \"\"\"This function will calculate the erorrs using mean absolute erorrs.\n",
    "    \n",
    "    Input:\n",
    "    pred_dict -> dict (key: unique_key, values: y and yhat)\n",
    "    actual -> (values: the actual y value)\n",
    "    yhat -> (values: the predicdted value from the model)\n",
    "    \n",
    "    Output:\n",
    "    error -> int (mean absolute erorr)\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    error_total = 0\n",
    "    # looping through the dictionary and summing the error.\n",
    "    for key in pred_dict:\n",
    "        error = mean_absolute_error(pred_dict[key][actual], pred_dict[key][yhat])\n",
    "        error_total += error\n",
    "        \n",
    "    return error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c83b71",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52686777",
   "metadata": {},
   "source": [
    "- Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f431d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prophet_pred = create_prophet(train_df, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = combine_y_yhat(train_df, prophet_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebe539",
   "metadata": {},
   "source": [
    "Combine the results from the Prophet model and the actual y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ea028",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_groups = find_mae(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e231b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Using Prophet, the mean absolute error in the training dataset is {round(train_error, 4)}\")\n",
    "print(f\"Using Prophet, the mean absolute error in the validation dataset is {round(test_error, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e68e7",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/prophet_model/combined_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da6a6b",
   "metadata": {},
   "source": [
    "Export the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e112fe1",
   "metadata": {},
   "source": [
    "Get unique key and obtain mean absolute error by train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa91ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df =  pd.DataFrame.from_records(error_by_groups).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.columns = ['unique_key', 'train_error', 'test_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_error = np.sum(error_df.test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using Prophet, the mean absolute error in the training dataset is {round(total_test_error, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed70e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaddc95",
   "metadata": {},
   "source": [
    "### Question: Will the model improve if we remove 0 sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005579fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales_summary = train_df.groupby('unique_store_dep_key').sum()[['sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e80fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_sales_summary[total_sales_summary.sales == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e4c62",
   "metadata": {},
   "source": [
    "There are actually a good amount of 0 sales. Let's try to remove them and see if the error gets lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252f2ac",
   "metadata": {},
   "source": [
    "- filter 0 sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sales_dep = list(total_sales_summary[total_sales_summary.sales == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(zero_sales_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_pred_no_zeros = create_prophet_remove_zero(train_df, zero_sales_dep, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict_no_zero = combine_y_yhat(train_df, prophet_pred_no_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f60c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_by_groups_no_zero = find_mae(combined_dict_no_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccdb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df_no_zeros =  pd.DataFrame.from_records(error_by_groups_no_zero).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df_no_zeros.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df_no_zeros.columns = ['unique_key', 'train_error', 'test_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94669a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_error = np.sum(error_df.test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_error_no_zeros = np.sum(error_df_no_zeros.test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f5585",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_test_error_no_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc5e73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(unique_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239261f1",
   "metadata": {},
   "source": [
    "There is actually no difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577608b",
   "metadata": {},
   "source": [
    "Dealing with seasonality will be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba4d6c",
   "metadata": {},
   "source": [
    "Trying to aggregate the sum of sales based on stroe number."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
