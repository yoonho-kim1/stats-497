{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58e0d34",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a24e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea146f",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a0e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../asset/train_df.pkl')\n",
    "test_df = pd.read_pickle('../asset/test_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3cb2a",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0df573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_features(data):\n",
    "    \n",
    "    \"\"\"This function will help us to convert boolean and cateogrical values to numerical values.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    \n",
    "    Output:\n",
    "    final_df -> dataframe (after feature conversions including one hot encoding and ordinal encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting features that we will use for the Prophet model\n",
    "    features = list(data.columns)\n",
    "    \n",
    "    # treat train and test dataset in a different way\n",
    "    if 'sales' in data.columns:\n",
    "        features.append('sales')\n",
    "        \n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    not_transformed = []\n",
    "    no_need_to_transform = []\n",
    "    \n",
    "    \n",
    "    # based on the features, use different methods for encoders. \n",
    "    for col, dtype in data.dtypes.items():\n",
    "        if col in ['date', 'unique_key', 'date_year', 'date_quarter', 'date_month', 'date_day', 'date_week', 'year_month'] or dtype in ['datetime64[ns]', 'timedelta64[ns]']:\n",
    "            no_need_to_transform.append(col)\n",
    "        elif col == 'transferred':\n",
    "            data.loc[:, col] = data[col].apply(lambda x: 1 if x == True else 0)\n",
    "        elif col in ['state_sales_cut', 'store_sales_bins', 'store_type_sales',\n",
    "                     'family_sales_bins', 'onpromo_avg_bins', 'cluster_sales_indicator']:\n",
    "            data.loc[:, col] = oe.fit_transform(data[col].values.reshape(-1,1))\n",
    "        else:\n",
    "            not_transformed.append(col) \n",
    "    dummy_df = pd.get_dummies(data.drop(columns = no_need_to_transform))\n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22891926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split(data, train_size):\n",
    "    \n",
    "    \"\"\"This function will help us create train test split in time series.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    train_size -> float (the percentage of train_set in our dataset)\n",
    "    \n",
    "    Output:\n",
    "    train_df -> dataframe (train_set based on the train_size)\n",
    "    test_df -> dataframe (test_set based on the train_size)\"\"\"\n",
    "    \n",
    "    total_row = len(data)\n",
    "    train_idx = int(total_row * train_size)\n",
    "    \n",
    "    train = data[:train_idx]\n",
    "    test = data[train_idx:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8b9f8",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e799a6",
   "metadata": {},
   "source": [
    "For the 0 sales stores, we will use 0 as our predictions. Otherwise, we will use pretrained XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "164649be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary to save the results\n",
    "pred_dict = {}\n",
    "\n",
    "# change special characters\n",
    "train_df.loc[:, 'family'] = train_df.family.apply(lambda x: str(x).replace('/', '&'))\n",
    "test_df.loc[:, 'family'] = test_df.family.apply(lambda x: str(x).replace('/', '&'))\n",
    "\n",
    "# create unique keys based on the store number and departments\n",
    "test_df.loc[:, 'unique_str_dep_key'] = test_df.store_nbr.apply(lambda x: str(x)) + '-' + test_df.family.apply(lambda x: str(x))\n",
    "train_df.loc[:, 'unique_str_dep_key'] = train_df.store_nbr.apply(lambda x: str(x)) + '-' + train_df.family.apply(lambda x: str(x))\n",
    "\n",
    "# filter out zero sales\n",
    "total_sales = train_df.groupby('unique_str_dep_key').sum()[['sales']]\n",
    "zero_sales = total_spales[total_sales.sales == 0]\n",
    "zero_sales_list = list(zero_sales.index)\n",
    "\n",
    "unique_key_list = list(train_df.unique_str_dep_key.unique())\n",
    "# loop through the unique key\n",
    "for u_key in unique_key_list:\n",
    "    \n",
    "    # filter dataframe so that we can only get the unique df\n",
    "    data = train_df[train_df.unique_str_dep_key == u_key]\n",
    "    pred_df = test_df[test_df.unique_str_dep_key == u_key]\n",
    "    data.reset_index().drop(columns = ['index', 'date', 'store_nbr', 'family'])\n",
    "    pred_df.reset_index().drop(columns = ['index', 'date', 'store_nbr', 'family'])\n",
    "    \n",
    "    # perform feature engineering -> one hot coding\n",
    "    new_data = convert_all_features(data)\n",
    "    new_pred_data = convert_all_features(pred_df.drop(columns= ['date']))\n",
    "    new_pred_data = new_pred_data.set_index('id')\n",
    "    \n",
    "    # train test split (70:30)\n",
    "    train, test = time_split(new_data, 0.7)\n",
    "    train.drop_duplicates(inplace = True)\n",
    "    test.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # assign predictors and target variables\n",
    "    train_X, train_y = train.drop(columns = ['sales']), train.sales.values\n",
    "    test_X, test_y = test.drop(columns = ['sales']), test.sales.values\n",
    "    \n",
    "    total_row = len(new_pred_data)\n",
    "    \n",
    "    if u_key not in zero_sales_list:\n",
    "        with open(f'../asset/best_params_optuna/best_params_{u_key}.pkl', 'rb') as f:\n",
    "            best_params = pickle.load(f)\n",
    "            \n",
    "        for k, v in best_params.items():\n",
    "            if k in ['max_depth', 'min_child_weight', 'n_estimators']:\n",
    "                best_params[k] = int(v)\n",
    "\n",
    "        xgbr = XGBRegressor(**best_params)\n",
    "        xgbr.fit(train_X, train_y)\n",
    "        pred = xgbr.predict(new_pred_data)\n",
    "\n",
    "        pred_dict[u_key] = pred\n",
    "    \n",
    "    else:\n",
    "        zeros = [0 for _ in range(total_row)]\n",
    "        pred_dict[u_key] = zeros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
