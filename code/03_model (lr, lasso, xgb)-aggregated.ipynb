{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2912683b",
   "metadata": {},
   "source": [
    "Note: pip install tsfresh is not successful. tf not downloading correctly. assuming it's related to m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfca37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a01525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tsmoothie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d3f53",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282fd645",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/aggregated_train_df.pkl', 'rb') as f:\n",
    "    final_train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7b8e8",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_features(data):\n",
    "    \n",
    "    \"\"\"This function will help us to convert boolean and cateogrical values to numerical values.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    \n",
    "    Output:\n",
    "    final_df -> dataframe (after feature conversions including one hot encoding and ordinal encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting features that we will use for the Prophet model\n",
    "    features = list(data.columns)\n",
    "    \n",
    "    # treat train and test dataset in a different way\n",
    "    if 'final_sales' in data.columns:\n",
    "        features.append('final_sales')\n",
    "        \n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    not_transformed = []\n",
    "    no_need_to_transform = []\n",
    "    \n",
    "    \n",
    "    # based on the features, use different methods for encoders. \n",
    "    for col, dtype in data.dtypes.items():\n",
    "        if col in ['date', 'unique_key', 'date_year', 'date_quarter', 'date_month', 'date_day', 'date_week', 'year_month'] or dtype in ['datetime64[ns]', 'timedelta64[ns]']:\n",
    "            no_need_to_transform.append(col)\n",
    "        elif col == 'transferred':\n",
    "            data.loc[:, col] = data[col].apply(lambda x: 1 if x == True else 0)\n",
    "        elif col in ['state_sales_cut', 'store_sales_bins', 'store_type_sales',\n",
    "                     'family_sales_bins', 'onpromo_avg_bins', 'cluster_sales_indicator']:\n",
    "            data.loc[:, col] = oe.fit_transform(data[col].values.reshape(-1,1))\n",
    "        else:\n",
    "            not_transformed.append(col) \n",
    "    dummy_df = pd.get_dummies(data.drop(columns = no_need_to_transform))\n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split(data, train_size):\n",
    "    \n",
    "    \"\"\"This function will help us create train test split in time series.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    train_size -> float (the percentage of train_set in our dataset)\n",
    "    \n",
    "    Output:\n",
    "    train_df -> dataframe (train_set based on the train_size)\n",
    "    test_df -> dataframe (test_set based on the train_size)\"\"\"\n",
    "    \n",
    "    total_row = len(data)\n",
    "    train_idx = int(total_row * train_size)\n",
    "    \n",
    "    train = data[:train_idx]\n",
    "    test = data[train_idx:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9672923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_errors(pred_dict, actual, yhat):\n",
    "    \n",
    "    \"\"\"This function will calculate the erorrs using mean absolute erorrs.\n",
    "    \n",
    "    Input:\n",
    "    pred_dict -> dict (key: unique_key, values: y and yhat)\n",
    "    actual -> (values: the actual y value)\n",
    "    yhat -> (values: the predicdted value from the model)\n",
    "    \n",
    "    Output:\n",
    "    error -> int (mean absolute erorr)\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    error_total = 0\n",
    "    # looping through the dictionary and summing the error.\n",
    "    for key in pred_dict:\n",
    "        error = mean_absolute_error(pred_dict[key][actual], pred_dict[key][yhat])\n",
    "        error_total += error\n",
    "        \n",
    "    return error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08bb05",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f2021",
   "metadata": {},
   "source": [
    "- Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162d59c",
   "metadata": {},
   "source": [
    "It will not be the best model, but we can start building a simple model more for the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to save the results\n",
    "lr_train_dict = {}\n",
    "lr_test_dict = {}\n",
    "\n",
    "# get the unique list of keys\n",
    "unique_key_list = list(final_train_df.store_nbr.unique())\n",
    "\n",
    "# we want to filter out data based on the unique keys\n",
    "for key in unique_key_list:\n",
    "    \n",
    "    # filter out data\n",
    "    data = final_train_df[final_train_df.store_nbr == key]\n",
    "    data = data.reset_index().drop(columns = ['index', 'date', 'store_nbr'])\n",
    "    \n",
    "    # perform feature engineering -> one hot coding\n",
    "    new_data = convert_all_features(data)\n",
    "    \n",
    "    # train test split (70:30)\n",
    "    train, test = time_split(new_data, 0.7)\n",
    "    train.drop_duplicates(inplace = True)\n",
    "    test.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # assign predictors and target variables\n",
    "    train_X, train_y = train.drop(columns = ['total_sales']), train.total_sales.values\n",
    "    test_X, test_y = test.drop(columns = ['total_sales']), test.total_sales.values\n",
    "    \n",
    "    # initiate the linear regression model\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    \n",
    "    # start training LinearRegression \n",
    "    lr.fit(train_X, train_y)\n",
    "    \n",
    "    # train y hat and test y hat\n",
    "    lr_train_pred = lr.predict(train_X)\n",
    "    lr_test_pred = lr.predict(test_X)\n",
    "    \n",
    "    # save the results in the dictionaries\n",
    "    if key not in lr_train_dict:\n",
    "        lr_train_dict[key] = {\n",
    "            'train_y':train_y,\n",
    "            'train_yhat':lr_train_pred\n",
    "        }\n",
    "    \n",
    "    if key not in lr_test_dict:\n",
    "        lr_test_dict[key] = {\n",
    "            'test_y':test_y,\n",
    "            'test_yhat':lr_test_pred}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62578922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to save the results\n",
    "lr_train_dict = {}\n",
    "lr_test_dict = {}\n",
    "\n",
    "# get the unique list of keys\n",
    "unique_key_list = list(final_train_df.store_nbr.unique())\n",
    "\n",
    "# we want to filter out data based on the unique keys\n",
    "for key in unique_key_list:\n",
    "    \n",
    "    # filter out data\n",
    "    data = final_train_df[final_train_df.store_nbr == key]\n",
    "    data = data.reset_index().drop(columns = ['index', 'date', 'store_nbr'])\n",
    "    \n",
    "    # perform feature engineering -> one hot coding\n",
    "    new_data = convert_all_features(data)\n",
    "    \n",
    "    # train test split (70:30)\n",
    "    train, test = time_split(new_data, 0.7)\n",
    "    train.drop_duplicates(inplace = True)\n",
    "    test.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # assign predictors and target variables\n",
    "    train_X, train_y = train.drop(columns = ['total_sales']), train.total_sales.values\n",
    "    test_X, test_y = test.drop(columns = ['total_sales']), test.total_sales.values\n",
    "    \n",
    "    # initiate the linear regression model\n",
    "    lr = XGBRegressor()\n",
    "    \n",
    "    \n",
    "    # start training LinearRegression \n",
    "    lr.fit(train_X, train_y)\n",
    "    \n",
    "    # train y hat and test y hat\n",
    "    lr_train_pred = lr.predict(train_X)\n",
    "    lr_test_pred = lr.predict(test_X)\n",
    "    \n",
    "    # save the results in the dictionaries\n",
    "    if key not in lr_train_dict:\n",
    "        lr_train_dict[key] = {\n",
    "            'train_y':train_y,\n",
    "            'train_yhat':lr_train_pred\n",
    "        }\n",
    "    \n",
    "    if key not in lr_test_dict:\n",
    "        lr_test_dict[key] = {\n",
    "            'test_y':test_y,\n",
    "            'test_yhat':lr_test_pred}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lr_test_dict['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8072f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame()\n",
    "\n",
    "for key in lr_test_dict:\n",
    "    temp = pd.DataFrame(lr_test_dict[key])\n",
    "    temp.loc[:, 'store_nbr'] = key\n",
    "    \n",
    "    # create lists to distinguish the difference between train and test\n",
    "    total_len = len(temp)\n",
    "    train_idx = int(total_len * 0.7)\n",
    "    test_idx = total_len - train_idx\n",
    "\n",
    "    # create indicators based on the length of data -> train and test\n",
    "    train_ind_list = ['train' for _ in range(train_idx)]\n",
    "    test_ind_list = ['test' for _ in range(test_idx)]\n",
    "    combined_ind_list = train_ind_list + test_ind_list\n",
    "    temp.loc[:, 'indicator'] = combined_ind_list\n",
    "    total_df = total_df.append(temp, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993edf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.loc[:, 'mae'] = np.abs(total_df['test_y'] - total_df['test_yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(total_df.groupby('store_nbr').sum()[['mae']]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.21845254e+08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00f83e",
   "metadata": {},
   "source": [
    "- Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435ea75",
   "metadata": {},
   "source": [
    "We can use Lasso to find out which features are more important in terms of building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to save the results\n",
    "lasso_coef_dict = {}\n",
    "lasso_train_dict = {}\n",
    "lasso_test_dict = {}\n",
    "\n",
    "# create unique keys based on the store number and departments\n",
    "train_df.loc[:, 'unique_str_dep_key'] = train_df.store_nbr.apply(lambda x: str(x)) + '-' + train_df.family.apply(lambda x: str(x))\n",
    "\n",
    "# get the unique list of keys\n",
    "unique_key_list = list(train_df.unique_str_dep_key.unique())\n",
    "\n",
    "# we want to filter out data based on the unique keys\n",
    "for key in unique_key_list:\n",
    "    \n",
    "    # filter out data\n",
    "    data = train_df[train_df.unique_str_dep_key == key]\n",
    "    data = data.reset_index().drop(columns = ['index', 'date', 'unique_str_dep_key', 'store_nbr', 'family'])\n",
    "    \n",
    "    # perform feature engineering -> one hot coding\n",
    "    new_data = convert_all_features(data)\n",
    "    \n",
    "    # train test split (70:30)\n",
    "    train, test = time_split(new_data, 0.7)\n",
    "    train.drop_duplicates(inplace = True)\n",
    "    test.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # assign predictors and target variables\n",
    "    train_X, train_y = train.drop(columns = ['sales']), train.sales.values\n",
    "    test_X, test_y = test.drop(columns = ['sales']), test.sales.values\n",
    "    \n",
    "    # initiate the linear regression model\n",
    "    lasso = Lasso(alpha = 0.01)\n",
    "    \n",
    "    # start training\n",
    "    lasso.fit(train_X, train_y)\n",
    "    \n",
    "    # train y hat and test y hat\n",
    "    lasso_train_pred = lasso.predict(train_X)\n",
    "    lasso_test_pred = lasso.predict(test_X)\n",
    "    \n",
    "    # save the results in the dictionaries\n",
    "    if key not in lasso_train_dict:\n",
    "        lasso_train_dict[key] = {\n",
    "            'train_y':train_y,\n",
    "            'train_yhat':lasso_train_pred\n",
    "        }\n",
    "    \n",
    "    if key not in lasso_test_dict:\n",
    "        lasso_test_dict[key] = {\n",
    "            'test_y':test_y,\n",
    "            'test_yhat':lasso_test_pred}\n",
    "        \n",
    "    if key not in lasso_coef_dict:\n",
    "        lasso_coef_dict[key] = {'features':lasso.feature_names_in_,\n",
    "                                'coef':lasso.coef_}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c952d",
   "metadata": {},
   "source": [
    "- XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb71bef",
   "metadata": {},
   "source": [
    "Unlike linear regressio, XGB does not need any assumptions regarding the data (non-parametric) and often perform well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d248a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to save the results\n",
    "\n",
    "xgb_train_dict = {}\n",
    "xgb_test_dict = {}\n",
    "\n",
    "# create unique keys based on the store number and departments\n",
    "train_df.loc[:, 'unique_str_dep_key'] = train_df.store_nbr.apply(lambda x: str(x)) + '-' + train_df.family.apply(lambda x: str(x))\n",
    "\n",
    "# get the unique list of keys\n",
    "unique_key_list = list(train_df.unique_str_dep_key.unique())\n",
    "\n",
    "# we want to filter out data based on the unique keys\n",
    "for key in unique_key_list:\n",
    "    \n",
    "    \n",
    "    # filter out data\n",
    "    data = train_df[train_df.unique_str_dep_key == key]\n",
    "    data = data.reset_index().drop(columns = ['index', 'date', 'unique_str_dep_key', 'store_nbr', 'family'])\n",
    "    \n",
    "    # perform feature engineering \n",
    "    new_data = convert_all_features(data)\n",
    "    \n",
    "    # train test split (70:30)\n",
    "    train, test = time_split(new_data, 0.7)\n",
    "    train.drop_duplicates(inplace = True)\n",
    "    test.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # assign predictors and target variables\n",
    "    train_X, train_y = train.drop(columns = ['sales']), train.sales.values\n",
    "    test_X, test_y = test.drop(columns = ['sales']), test.sales.values\n",
    "    \n",
    "    # initiate the xgboost\n",
    "    xgbr = XGBRegressor()\n",
    "    \n",
    "    # start training\n",
    "    xgbr.fit(train_X, train_y)\n",
    "    \n",
    "    # train y hat and test y hat\n",
    "    xgb_train_pred = xgbr.predict(train_X)\n",
    "    xgb_test_pred = xgbr.predict(test_X)\n",
    "    \n",
    "    # save the results in the dictionaries\n",
    "    if key not in xgb_train_dict:\n",
    "        xgb_train_dict[key] = {\n",
    "            'train_y':train_y,\n",
    "            'train_yhat':xgb_train_pred\n",
    "        }\n",
    "    \n",
    "    if key not in xgb_test_dict:\n",
    "        xgb_test_dict[key] = {\n",
    "            'test_y':test_y,\n",
    "            'test_yhat':xgb_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afb7f6",
   "metadata": {},
   "source": [
    "- combine two (Lasso + XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec261c8",
   "metadata": {},
   "source": [
    "Let's use Lasso as our feature selection, then use XGBoost for the better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c5509",
   "metadata": {},
   "source": [
    "1. Lasso for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to save the results\n",
    "lasso_coef_dict = {}\n",
    "lasso_train_dict = {}\n",
    "lasso_test_dict = {}\n",
    "\n",
    "# create unique keys based on the store number and departments\n",
    "train_df.loc[:, 'unique_str_dep_key'] = train_df.store_nbr.apply(lambda x: str(x)) + '-' + train_df.family.apply(lambda x: str(x))\n",
    "\n",
    "# get the unique list of keys\n",
    "unique_key_list = list(train_df.unique_str_dep_key.unique())\n",
    "\n",
    "# we want to filter out data based on the unique keys\n",
    "for key in unique_key_list:\n",
    "    \n",
    "    # filter out data\n",
    "    data = train_df[train_df.unique_str_dep_key == key]\n",
    "    data = data.reset_index().drop(columns = ['index', 'date', 'unique_str_dep_key', 'store_nbr', 'family'])\n",
    "    \n",
    "    # perform feature engineering -> one hot coding\n",
    "    new_data = convert_all_features(data)\n",
    "    \n",
    "    # train test split (70:30)\n",
    "    train, test = time_split(new_data, 0.7)\n",
    "    train.drop_duplicates(inplace = True)\n",
    "    test.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # assign predictors and target variables\n",
    "    train_X, train_y = train.drop(columns = ['sales']), train.sales.values\n",
    "    test_X, test_y = test.drop(columns = ['sales']), test.sales.values\n",
    "    \n",
    "    # initiate the linear regression model\n",
    "    lasso = Lasso(alpha = 0.01)\n",
    "    \n",
    "    # start training\n",
    "    lasso.fit(train_X, train_y)\n",
    "    \n",
    "    # train y hat and test y hat\n",
    "    lasso_train_pred = lasso.predict(train_X)\n",
    "    lasso_test_pred = lasso.predict(test_X)\n",
    "    \n",
    "    # save the results in the dictionaries\n",
    "    if key not in lasso_train_dict:\n",
    "        lasso_train_dict[key] = {\n",
    "            'train_y':train_y,\n",
    "            'train_yhat':lasso_train_pred\n",
    "        }\n",
    "    \n",
    "    if key not in lasso_test_dict:\n",
    "        lasso_test_dict[key] = {\n",
    "            'test_y':test_y,\n",
    "            'test_yhat':lasso_test_pred}\n",
    "        \n",
    "    if key not in lasso_coef_dict:\n",
    "        lasso_coef_dict[key] = {'features':lasso.feature_names_in_,\n",
    "                                'coef':lasso.coef_}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494669de",
   "metadata": {},
   "source": [
    "2. Use XGBoost for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_dict = {}\n",
    "total_test_dict = {}\n",
    "\n",
    "\n",
    "for key, val in lasso_coef_dict.items():\n",
    "            \n",
    "        # filter out data\n",
    "        data = train_df[train_df.unique_str_dep_key == key]\n",
    "        data.reset_index().drop(columns = ['index', 'date', 'store_nbr', 'family'])\n",
    "\n",
    "        # perform feature engineering -> one hot coding\n",
    "        new_data = convert_all_features(data)\n",
    "\n",
    "        # train test split (70:30)\n",
    "        train, test = time_split(new_data, 0.7)\n",
    "        train.drop_duplicates(inplace = True)\n",
    "        test.drop_duplicates(inplace = True)\n",
    "        \n",
    "        if np.sum(val['coef']) == 0:\n",
    "            \n",
    "            # use xgboost directly\n",
    "            \n",
    "            # assign predictors and target variables\n",
    "            train_X, train_y = train.drop(columns = ['sales']), train.sales.values\n",
    "            test_X, test_y = test.drop(columns = ['sales']), test.sales.values\n",
    "\n",
    "            xgbr = XGBRegressor()\n",
    "\n",
    "            # start training\n",
    "            xgbr.fit(train_X, train_y)\n",
    "\n",
    "            # train y hat and test y hat\n",
    "            xgb_train_pred = xgbr.predict(train_X)\n",
    "            xgb_test_pred = xgbr.predict(test_X)\n",
    "\n",
    "            total_train_dict[key] = {'actual':train_y,\n",
    "                                     'yhat':xgb_train_pred}\n",
    "\n",
    "            total_test_dict[key] = {'actual':test_y,\n",
    "                                    'yhat':xgb_test_pred}\n",
    "\n",
    "        \n",
    "\n",
    "        else:\n",
    "            lasso_df = pd.DataFrame(v)\n",
    "            # drop features that are not important\n",
    "            lasso_df = lasso_df[lasso_df.coef != 0]\n",
    "            filtered_features = list(lasso_df.features)\n",
    "            filtered_features.append('sales')\n",
    "            \n",
    "            # select features \n",
    "            filtered_train = train[filtered_features]\n",
    "            filtered_test = test[filtered_features]\n",
    "\n",
    "            # assign predictors and target variables\n",
    "            train_X, train_y = filtered_train.drop(columns = ['sales']), train.sales.values\n",
    "            test_X, test_y = filtered_test.drop(columns = ['sales']), test.sales.values\n",
    "\n",
    "            xgbr = XGBRegressor()\n",
    "\n",
    "            # start training\n",
    "            xgbr.fit(train_X, train_y)\n",
    "\n",
    "            # train y hat and test y hat\n",
    "            xgb_train_pred = xgbr.predict(train_X)\n",
    "            xgb_test_pred = xgbr.predict(test_X)\n",
    "\n",
    "            total_train_dict[key] = {'actual':train_y,\n",
    "                                     'yhat':xgb_train_pred}\n",
    "\n",
    "            total_test_dict[key] = {'actual':test_y,\n",
    "                                    'yhat':xgb_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5698e1",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff5bb4",
   "metadata": {},
   "source": [
    "- linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/lr_model/lr_train_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_train_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf61d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/lr_model/lr_test_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25fa9f",
   "metadata": {},
   "source": [
    "- lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/lasso_model/lasso_train_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(lasso_train_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/lasso_model/lasso_test_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(lasso_test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0f149",
   "metadata": {},
   "source": [
    "- xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ce06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/xgb_model/xgb_train_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_train_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab99f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/xgb_model/xgb_test_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_test_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004136e",
   "metadata": {},
   "source": [
    "- combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/combined_model/total_train_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(total_train_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../asset/combined_model/total_test_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(total_test_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
