{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e05ff",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c40e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, RepeatedKFold\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from optuna import create_study\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60be321",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c601bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../asset/train_df.pkl')\n",
    "test_df = pd.read_pickle('../asset/test_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8e43d",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8af2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_features(data):\n",
    "    \n",
    "    \"\"\"This function will help us to convert boolean and cateogrical values to numerical values.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    \n",
    "    Output:\n",
    "    final_df -> dataframe (after feature conversions including one hot encoding and ordinal encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    # selecting features that we will use for the Prophet model\n",
    "    features = list(data.columns)\n",
    "    \n",
    "    # treat train and test dataset in a different way\n",
    "    if 'sales' in data.columns:\n",
    "        features.append('sales')\n",
    "        \n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    not_transformed = []\n",
    "    no_need_to_transform = []\n",
    "    \n",
    "    \n",
    "    # based on the features, use different methods for encoders. \n",
    "    for col, dtype in data.dtypes.items():\n",
    "        if col in ['date', 'unique_key', 'date_year', 'date_quarter', 'date_month', 'date_day', 'date_week', 'year_month'] or dtype in ['datetime64[ns]', 'timedelta64[ns]']:\n",
    "            no_need_to_transform.append(col)\n",
    "        elif col == 'transferred':\n",
    "            data.loc[:, col] = data[col].apply(lambda x: 1 if x == True else 0)\n",
    "        elif col in ['state_sales_cut', 'store_sales_bins', 'store_type_sales',\n",
    "                     'family_sales_bins', 'onpromo_avg_bins', 'cluster_sales_indicator']:\n",
    "            data.loc[:, col] = oe.fit_transform(data[col].values.reshape(-1,1))\n",
    "        else:\n",
    "            not_transformed.append(col) \n",
    "    dummy_df = pd.get_dummies(data.drop(columns = no_need_to_transform))\n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split(data, train_size):\n",
    "    \n",
    "    \"\"\"This function will help us create train test split in time series.\n",
    "    \n",
    "    Input:\n",
    "    data -> dataframe (the original dataframe)\n",
    "    train_size -> float (the percentage of train_set in our dataset)\n",
    "    \n",
    "    Output:\n",
    "    train_df -> dataframe (train_set based on the train_size)\n",
    "    test_df -> dataframe (test_set based on the train_size)\"\"\"\n",
    "    \n",
    "    total_row = len(data)\n",
    "    train_idx = int(total_row * train_size)\n",
    "    \n",
    "    train = data[:train_idx]\n",
    "    test = data[train_idx:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_errors(pred_dict, actual, yhat):\n",
    "    \n",
    "    \"\"\"This function will calculate the erorrs using mean absolute erorrs.\n",
    "    \n",
    "    Input:\n",
    "    pred_dict -> dict (key: unique_key, values: y and yhat)\n",
    "    actual -> (values: the actual y value)\n",
    "    yhat -> (values: the predicdted value from the model)\n",
    "    \n",
    "    Output:\n",
    "    error -> int (mean absolute erorr)\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    error_total = 0\n",
    "    # looping through the dictionary and summing the error.\n",
    "    for key in pred_dict:\n",
    "        error = mean_absolute_error(pred_dict[key][actual], pred_dict[key][yhat])\n",
    "        error_total += error\n",
    "        \n",
    "    return error_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9846d31",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99246c8",
   "metadata": {},
   "source": [
    "- filter out zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237028bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dictionaries to save the results\n",
    "xgb_params_dict = {}\n",
    "\n",
    "# create unique keys based on the store number and departments\n",
    "train_df.loc[:, 'unique_str_dep_key'] = train_df.store_nbr.apply(lambda x: str(x)) + '-' + train_df.family.apply(lambda x: str(x))\n",
    "\n",
    "# get the unique list of keys\n",
    "unique_key_list = list(train_df.unique_str_dep_key.unique())\n",
    "\n",
    "# filter stores that have 0 sales\n",
    "total_sales_df = train_df.groupby('unique_str_dep_key').sum()[['sales']]\n",
    "zero_sales = total_sales_df[total_sales_df.sales == 0]\n",
    "\n",
    "# find stores that have zero sales total\n",
    "zero_list = list(zero_sales.index)\n",
    "\n",
    "# we want to filter out data based on the unique keys\n",
    "for key in unique_key_list:\n",
    "    \n",
    "    if key not in zero_list:\n",
    "        # filter out data\n",
    "        data = train_df[train_df.unique_str_dep_key == key]\n",
    "        data.reset_index().drop(columns = ['index', 'date', 'store_nbr', 'family'])\n",
    "\n",
    "        # perform feature engineering -> one hot coding\n",
    "        new_data = convert_all_features(data)\n",
    "\n",
    "        # train test split (70:30)\n",
    "        train, test = time_split(new_data, 0.7)\n",
    "        train.drop_duplicates(inplace = True)\n",
    "        test.drop_duplicates(inplace = True)\n",
    "\n",
    "        # assign predictors and target variables\n",
    "        train_X, train_y = train.drop(columns = ['sales']), train.sales.values\n",
    "        test_X, test_y = test.drop(columns = ['sales']), test.sales.values\n",
    "        \n",
    "        # define params\n",
    "        params = {'max_depth': [3, 6, 10, 15],\n",
    "              'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n",
    "              'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "              'colsample_bytree': np.arange(0.5, 1.0, 0.1),\n",
    "              'colsample_bylevel': np.arange(0.5, 1.0, 0.1),\n",
    "              'n_estimators': [100, 250, 500, 750]\n",
    "              }\n",
    "        \n",
    "        # start training\n",
    "        xgbclf = XGBRegressor(tree_method='hist')\n",
    "        clf = RandomizedSearchCV(estimator=xgbclf,\n",
    "                             param_distributions=params,\n",
    "                             n_iter=25,\n",
    "                             n_jobs=4,\n",
    "                             verbose=1)\n",
    "        # fit the model\n",
    "        clf.fit(train_X, train_y)\n",
    "        \n",
    "        # find the best params\n",
    "        best_hyperparams = clf.best_params_\n",
    "        \n",
    "        print(f\"{key} found best params.\")\n",
    "        \n",
    "        # save the best results\n",
    "        xgb_params_dict[key] = best_hyperparams\n",
    "        key_change = str(key).replace('/', '_')\n",
    "        with open(f'../asset/best_params/best_params_{key_change}.pkl', 'wb') as f:\n",
    "            pickle.dump(xgb_params_dict[key], f)\n",
    "        \n",
    "    else:\n",
    "        # meaning these store + dep combinations have 0 sales\n",
    "        xgb_params_dict[key] = 'zero_coef'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6b060",
   "metadata": {},
   "source": [
    "### Use hyper parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48759aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to save the results\n",
    "xgb_pred_dict_train = {}\n",
    "xgb_pred_dict_test = {}\n",
    "\n",
    "# create unique keys based on the store number and departments\n",
    "train_df.loc[:, 'unique_str_dep_key'] = train_df.store_nbr.apply(lambda x: str(x)) + '-' + train_df.family.apply(lambda x: str(x))\n",
    "\n",
    "# get the unique list of keys\n",
    "unique_key_list = list(train_df.unique_str_dep_key.unique())\n",
    "\n",
    "# filter stores that have 0 sales\n",
    "total_sales_df = train_df.groupby('unique_str_dep_key').sum()[['sales']]\n",
    "zero_sales = total_sales_df[total_sales_df.sales == 0]\n",
    "\n",
    "# find stores that have zero sales total\n",
    "zero_list = list(zero_sales.index)\n",
    "\n",
    "# we want to filter out data based on the unique keys\n",
    "for key, val in xgb_params_dict.items():\n",
    "    \n",
    "    # filter dataframe so that we can only get the unique df\n",
    "    data = train_df[train_df.unique_str_dep_key == key]\n",
    "    data.reset_index().drop(columns = ['index', 'date', 'store_nbr', 'family'])\n",
    "\n",
    "    # perform feature engineering -> one hot coding\n",
    "    new_data = convert_all_features(data)\n",
    "\n",
    "    # train test split (70:30)\n",
    "    train, test = time_split(new_data, 0.7)\n",
    "    train.drop_duplicates(inplace = True)\n",
    "    test.drop_duplicates(inplace = True)\n",
    "\n",
    "    # assign predictors and target variables\n",
    "    train_X, train_y = train.drop(columns = ['sales']), train.sales.values\n",
    "    test_X, test_y = test.drop(columns = ['sales']), test.sales.values\n",
    "    \n",
    "    # if coef 0, then assign 0\n",
    "    if val == 'zero_coef':\n",
    "        xgb_pred_dict_train[key] = {'actual':train_y,\n",
    "                               'yhat':0}\n",
    "        xgb_pred_dict_test[key] = {'actual':test_y,\n",
    "                              'yhat':0}\n",
    "    # if not, use the hyper parameters that we created    \n",
    "    else:\n",
    "        # load the hyper parameters\n",
    "        key_change = str(key).replace('/', '_')\n",
    "        with open(f'../asset/best_params/best_params_{key_change}.pkl', 'rb') as f:\n",
    "            best_params = pickle.load(f)\n",
    "        \n",
    "        xgbr = XGBRegressor(**best_params)\n",
    "        xgbr.fit(train_X, train_y)\n",
    "        \n",
    "        train_y_hat = xgbr.predict(train_X)\n",
    "        test_y_hat = xgbr.predict(test_X)\n",
    "        \n",
    "        xgb_pred_dict_train[key] = {'actual':train_y,\n",
    "                               'yhat':train_y_hat}\n",
    "        xgb_pred_dict_test[key] = {'actual':test_y,\n",
    "                              'yhat':test_y_hat}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7a320",
   "metadata": {},
   "source": [
    "Combine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bdd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame()\n",
    "for key in xgb_pred_dict_test:\n",
    "    xgb_train_df = pd.DataFrame(xgb_pred_dict_train[key])\n",
    "    xgb_test_df = pd.DataFrame(xgb_pred_dict_test[key])\n",
    "    \n",
    "    xgb_train_df.loc[:, 'ind'] = 'train'\n",
    "    xgb_test_df.loc[:, 'ind'] = 'test'\n",
    "    \n",
    "    xgb_df = pd.concat([xgb_train_df, xgb_test_df])\n",
    "    xgb_df.loc[:, 'error'] = np.abs(xgb_df['actual'] - xgb_df['yhat'])\n",
    "    xgb_df.loc[:, 'unique_key'] = key\n",
    "    \n",
    "    total_df = total_df.append(xgb_df, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4de7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error_by_groups = total_df.groupby(['unique_key', 'ind']).sum()[['error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be178d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error_by_groups = total_error_by_groups.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(total_error_by_groups[total_error_by_groups.ind == 'train']['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(total_error_by_groups[total_error_by_groups.ind == 'test']['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc6e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
